{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95NTckuFZZzm"
   },
   "source": [
    "## package installation and load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UO71IBS6ZgZV"
   },
   "outputs": [],
   "source": [
    "### packages required \n",
    "!pip install fair-esm \n",
    "!pip install torch\n",
    "!pip install tensorflow\n",
    "!pip install sklearn\n",
    "!pip install h5py\n",
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w3sxRX-RfzoB",
    "outputId": "04d2c245-8f5d-4d2f-c905-67d63942b70a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found\n"
     ]
    }
   ],
   "source": [
    "import esm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from keras.layers import Dropout, AveragePooling1D, MaxPooling1D\n",
    "from keras.models import Sequential,Model, load_model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint,LearningRateScheduler, EarlyStopping\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "    tf.config.experimental.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU') # set the deep learning with GPU \n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m91cA0H5w_eY"
   },
   "source": [
    "### peptide embeddings with differen pretrained model\n",
    "https://github.com/facebookresearch/esm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmb2VZKeZlWC"
   },
   "source": [
    "Explaination of the memeory usage of the following models\n",
    "\n",
    "sequence length > 900 \n",
    "\n",
    "2560 output dimension model might need 24 G GPU memory\n",
    "\n",
    "5129 output dimension model, (in our attempts, 40 GB GPU memory is not enough) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pl7XVx5HZsHf"
   },
   "outputs": [],
   "source": [
    "def esm_embeddings_320(esm2, esm2_alphabet, peptide_sequence_list):\n",
    "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
    "  #         or you have too many sequences for transformation in a single converting, \n",
    "  #         you computer might automatically kill the job.\n",
    "  import torch\n",
    "  import esm\n",
    "  import collections\n",
    "  import pandas as pd\n",
    "  import gc\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "  else:\n",
    "    device = torch.device(\"cpu\")\n",
    "  esm2 = esm2.eval().to(device)\n",
    "\n",
    "  batch_converter = esm2_alphabet.get_batch_converter()\n",
    "\n",
    "  # load the peptide sequence list into the bach_converter\n",
    "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
    "  batch_lens = (batch_tokens != esm2_alphabet.padding_idx).sum(1)\n",
    "  ## batch tokens are the embedding results of the whole data set\n",
    "\n",
    "  batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "  # Extract per-residue representations (on CPU)\n",
    "  with torch.no_grad():\n",
    "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
    "      # model'esm2_t6_8M_UR50D' only has 6 layers, and therefore repr_layers parameters is equal to 6\n",
    "      results = esm2(batch_tokens, repr_layers=[6], return_contacts=False)\n",
    "  token_representations = results[\"representations\"][6].cpu()\n",
    "\n",
    "  # Generate per-sequence representations via averaging\n",
    "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "  sequence_representations = []\n",
    "  for i, tokens_len in enumerate(batch_lens):\n",
    "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "  # save dataset\n",
    "  # sequence_representations is a list and each element is a tensor\n",
    "  embeddings_results = collections.defaultdict(list)\n",
    "  for i in range(len(sequence_representations)):\n",
    "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
    "      each_seq_rep = sequence_representations[i].tolist()\n",
    "      for each_element in each_seq_rep:\n",
    "          embeddings_results[i].append(each_element)\n",
    "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
    "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
    "  torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "  return embeddings_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sXIH6LOsV-6v"
   },
   "outputs": [],
   "source": [
    "def esm_embeddings_480(esm2, esm2_alphabet, peptide_sequence_list):\n",
    "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
    "  #         or you have too many sequences for transformation in a single converting, \n",
    "  #         you computer might automatically kill the job.\n",
    "  import torch\n",
    "  import esm\n",
    "  import collections\n",
    "  import pandas as pd\n",
    "  import gc\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "  else:\n",
    "    device = torch.device(\"cpu\")\n",
    "  esm2 = esm2.eval().to(device)\n",
    "\n",
    "  batch_converter = esm2_alphabet.get_batch_converter()\n",
    "\n",
    "  # load the peptide sequence list into the bach_converter\n",
    "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
    "  batch_lens = (batch_tokens != esm2_alphabet.padding_idx).sum(1)\n",
    "  ## batch tokens are the embedding results of the whole data set\n",
    "\n",
    "  batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "  # Extract per-residue representations (on CPU)\n",
    "  with torch.no_grad():\n",
    "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
    "      # model'esm2_t12_35M_UR50D' only has 12 layers, and therefore repr_layers parameters is equal to 12\n",
    "      results = esm2(batch_tokens, repr_layers=[12], return_contacts=False)\n",
    "  token_representations = results[\"representations\"][12].cpu()\n",
    "\n",
    "  # Generate per-sequence representations via averaging\n",
    "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "  sequence_representations = []\n",
    "  for i, tokens_len in enumerate(batch_lens):\n",
    "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "  # save dataset\n",
    "  # sequence_representations is a list and each element is a tensor\n",
    "  embeddings_results = collections.defaultdict(list)\n",
    "  for i in range(len(sequence_representations)):\n",
    "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
    "      each_seq_rep = sequence_representations[i].tolist()\n",
    "      for each_element in each_seq_rep:\n",
    "          embeddings_results[i].append(each_element)\n",
    "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
    "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
    "  torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "  return embeddings_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SN7A3bgzhFny"
   },
   "outputs": [],
   "source": [
    "def esm_embeddings_640(esm2, esm2_alphabet, peptide_sequence_list):\n",
    "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
    "  #         or you have too many sequences for transformation in a single converting, \n",
    "  #         you computer might automatically kill the job.\n",
    "  import torch\n",
    "  import esm\n",
    "  import collections\n",
    "  import pandas as pd\n",
    "  import gc\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "  else:\n",
    "    device = torch.device(\"cpu\")\n",
    "  esm2 = esm2.eval().to(device)\n",
    "\n",
    "  batch_converter = esm2_alphabet.get_batch_converter()\n",
    "\n",
    "  # load the peptide sequence list into the bach_converter\n",
    "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
    "  batch_lens = (batch_tokens != esm2_alphabet.padding_idx).sum(1)\n",
    "  ## batch tokens are the embedding results of the whole data set\n",
    "\n",
    "  batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "  # Extract per-residue representations (on CPU)\n",
    "  with torch.no_grad():\n",
    "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
    "      # model'esm2_t30_150M_UR50D' only has 30 layers, and therefore repr_layers parameters is equal to 30\n",
    "      results = esm2(batch_tokens, repr_layers=[30], return_contacts=False)\n",
    "  token_representations = results[\"representations\"][30].cpu()\n",
    "\n",
    "  # Generate per-sequence representations via averaging\n",
    "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "  sequence_representations = []\n",
    "  for i, tokens_len in enumerate(batch_lens):\n",
    "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "  # save dataset\n",
    "  # sequence_representations is a list and each element is a tensor\n",
    "  embeddings_results = collections.defaultdict(list)\n",
    "  for i in range(len(sequence_representations)):\n",
    "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
    "      each_seq_rep = sequence_representations[i].tolist()\n",
    "      for each_element in each_seq_rep:\n",
    "          embeddings_results[i].append(each_element)\n",
    "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
    "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
    "  torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "  return embeddings_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MdQIPEe2V3lB"
   },
   "outputs": [],
   "source": [
    "def esm_embeddings_1280(esm2, esm2_alphabet, peptide_sequence_list):\n",
    "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
    "  #         or you have too many sequences for transformation in a single converting, \n",
    "  #         you computer might automatically kill the job.\n",
    "  import torch\n",
    "  import esm\n",
    "  import collections\n",
    "  import pandas as pd\n",
    "  import gc\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "  else:\n",
    "    device = torch.device(\"cpu\")\n",
    "  esm2 = esm2.eval().to(device)\n",
    "\n",
    "  batch_converter = esm2_alphabet.get_batch_converter()\n",
    "\n",
    "  # load the peptide sequence list into the bach_converter\n",
    "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
    "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "  ## batch tokens are the embedding results of the whole data set\n",
    "\n",
    "  batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "  # Extract per-residue representations (on CPU)\n",
    "  with torch.no_grad():\n",
    "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
    "      # model'esm2_t33_650M_UR50D' only has 33 layers, and therefore repr_layers parameters is equal to 33\n",
    "      results = esm2(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "  token_representations = results[\"representations\"][33].cpu()\n",
    "\n",
    "  # Generate per-sequence representations via averaging\n",
    "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "  sequence_representations = []\n",
    "  for i, tokens_len in enumerate(batch_lens):\n",
    "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "  # save dataset\n",
    "  # sequence_representations is a list and each element is a tensor\n",
    "  embeddings_results = collections.defaultdict(list)\n",
    "  for i in range(len(sequence_representations)):\n",
    "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
    "      each_seq_rep = sequence_representations[i].tolist()\n",
    "      for each_element in each_seq_rep:\n",
    "          embeddings_results[i].append(each_element)\n",
    "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
    "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
    "  torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "  return embeddings_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xmr0OfcTWN05"
   },
   "outputs": [],
   "source": [
    "def esm_embeddings_2560(esm2, esm2_alphabet, peptide_sequence_list):\n",
    "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
    "  #         or you have too many sequences for transformation in a single converting, \n",
    "  #         you computer might automatically kill the job.\n",
    "  import torch\n",
    "  import esm\n",
    "  import collections\n",
    "  import pandas as pd\n",
    "  import gc\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "  else:\n",
    "    device = torch.device(\"cpu\")\n",
    "  esm2 = esm2.eval().to(device)\n",
    "\n",
    "  batch_converter = esm2_alphabet.get_batch_converter()\n",
    "\n",
    "  # load the peptide sequence list into the bach_converter\n",
    "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
    "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "  ## batch tokens are the embedding results of the whole data set\n",
    "\n",
    "  batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "  # Extract per-residue representations (on CPU)\n",
    "  with torch.no_grad():\n",
    "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
    "      # model'esm2_t36_3B_UR50D' only has 36 layers, and therefore repr_layers parameters is equal to 36\n",
    "      results = esm2(batch_tokens, repr_layers=[36], return_contacts=False)\n",
    "  token_representations = results[\"representations\"][36].cpu()\n",
    "\n",
    "  # Generate per-sequence representations via averaging\n",
    "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "  sequence_representations = []\n",
    "  for i, tokens_len in enumerate(batch_lens):\n",
    "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "  # save dataset\n",
    "  # sequence_representations is a list and each element is a tensor\n",
    "  embeddings_results = collections.defaultdict(list)\n",
    "  for i in range(len(sequence_representations)):\n",
    "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
    "      each_seq_rep = sequence_representations[i].tolist()\n",
    "      for each_element in each_seq_rep:\n",
    "          embeddings_results[i].append(each_element)\n",
    "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
    "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
    "  torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "  return embeddings_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8kjrdZscWULt"
   },
   "outputs": [],
   "source": [
    "def esm_embeddings_5120(esm2, esm2_alphabet, peptide_sequence_list):\n",
    "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
    "  #         or you have too many sequences for transformation in a single converting, \n",
    "  #         you computer might automatically kill the job.\n",
    "  import torch\n",
    "  import esm\n",
    "  import collections\n",
    "  import pandas as pd\n",
    "  import gc\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "  else:\n",
    "    device = torch.device(\"cpu\")\n",
    "  esm2 = esm2.eval().to(device)\n",
    "\n",
    "  batch_converter = esm2_alphabet.get_batch_converter()\n",
    "\n",
    "  # load the peptide sequence list into the bach_converter\n",
    "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
    "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "  ## batch tokens are the embedding results of the whole data set\n",
    "\n",
    "  batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "  # Extract per-residue representations (on CPU)\n",
    "  with torch.no_grad():\n",
    "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
    "      # model'esm2_t48_15B_UR50D' only has 48 layers, and therefore repr_layers parameters is equal to 48\n",
    "      results = esm2(batch_tokens, repr_layers=[48], return_contacts=False)\n",
    "  token_representations = results[\"representations\"][48].cpu()\n",
    "\n",
    "  # Generate per-sequence representations via averaging\n",
    "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "  sequence_representations = []\n",
    "  for i, tokens_len in enumerate(batch_lens):\n",
    "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "  # save dataset\n",
    "  # sequence_representations is a list and each element is a tensor\n",
    "  embeddings_results = collections.defaultdict(list)\n",
    "  for i in range(len(sequence_representations)):\n",
    "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
    "      each_seq_rep = sequence_representations[i].tolist()\n",
    "      for each_element in each_seq_rep:\n",
    "          embeddings_results[i].append(each_element)\n",
    "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
    "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
    "  torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "  return embeddings_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hz-ZO_mrnSsn"
   },
   "source": [
    "### connect with googledrive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FroNV0ZpnW0e",
    "outputId": "170d16e3-ec32-4dec-b67a-06a107ccacf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "drive  sample_data\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zOCDoACdowFc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/universal_allergenicity_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6JdFa_7ZGL7"
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQJ-0YPyiQhp"
   },
   "source": [
    "#### load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTetST5AiUAh",
    "outputId": "6d5cddfa-c0b9-4856-ea41-72ca5d0febca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from keras.layers import Dropout, AveragePooling1D, MaxPooling1D\n",
    "from keras.models import Sequential,Model, load_model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint,LearningRateScheduler, EarlyStopping\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "    tf.config.experimental.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU') # set the deep learning with GPU \n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RLR10hUsxca"
   },
   "source": [
    "## Sequence embeddings with different pretrained protein language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztgr6bl-YGDI"
   },
   "source": [
    "### 320 feature dimension embedding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "eogcsymmYJ6A",
    "outputId": "dbb7a14c-94fe-497a-96b9-2065f8aff31f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t6_8M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t6_8M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t6_8M_UR50D-contact-regression.pt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import esm\n",
    "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
    "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
    "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "\n",
    "\n",
    "# whole dataset loading and dataset splitting \n",
    "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "\n",
    "# generate the peptide embeddings\n",
    "sequence_list = dataset['sequence'] \n",
    "embeddings_results = pd.DataFrame()\n",
    "for seq in sequence_list:\n",
    "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
    "    tuple_sequence = tuple([seq,seq])\n",
    "    peptide_sequence_list = []\n",
    "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
    "    # employ ESM model for converting and save the converted data in csv format\n",
    "    one_seq_embeddings = esm_embeddings_320(model, alphabet, peptide_sequence_list)\n",
    "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
    "embeddings_results.to_csv('whole_sample_dataset_esm2_t6_8M_UR50D_unified_320_dimension.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3yzGDFEYfCY"
   },
   "source": [
    "### 480 feature dimension embedding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "SMpqoc-3YlCo",
    "outputId": "96dd0a45-d888-42ff-d3e0-6c099dd17a6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t12_35M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t12_35M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t12_35M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t12_35M_UR50D-contact-regression.pt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import esm\n",
    "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
    "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
    "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
    "model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "\n",
    "\n",
    "# whole dataset loading and dataset splitting \n",
    "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "\n",
    "# generate the peptide embeddings\n",
    "sequence_list = dataset['sequence'] \n",
    "embeddings_results = pd.DataFrame()\n",
    "for seq in sequence_list:\n",
    "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
    "    tuple_sequence = tuple([seq,seq])\n",
    "    peptide_sequence_list = []\n",
    "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
    "    # employ ESM model for converting and save the converted data in csv format\n",
    "    one_seq_embeddings = esm_embeddings_480(model, alphabet, peptide_sequence_list)\n",
    "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
    "embeddings_results.to_csv('whole_sample_dataset_esm2_t12_35M_UR50D_unified_480_dimension.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFyOy-19Yvil"
   },
   "source": [
    "### 640 feature dimension embedding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TXYWJ5EY4o-",
    "outputId": "69a2a65e-69b0-4948-b54d-b0ffda338f4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t30_150M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t30_150M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t30_150M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t30_150M_UR50D-contact-regression.pt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import esm\n",
    "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
    "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
    "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
    "model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "\n",
    "\n",
    "# whole dataset loading and dataset splitting \n",
    "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "\n",
    "# generate the peptide embeddings\n",
    "sequence_list = dataset['sequence'] \n",
    "embeddings_results = pd.DataFrame()\n",
    "for seq in sequence_list:\n",
    "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
    "    tuple_sequence = tuple([seq,seq])\n",
    "    peptide_sequence_list = []\n",
    "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
    "    # employ ESM model for converting and save the converted data in csv format\n",
    "    one_seq_embeddings = esm_embeddings_640(model, alphabet, peptide_sequence_list)\n",
    "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
    "embeddings_results.to_csv('whole_sample_dataset_esm2_t30_150M_UR50D_unified_640_dimension.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEdAluvxYzfJ"
   },
   "source": [
    "### 1280 feature dimension embedding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwang7k8Y_gt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import esm\n",
    "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
    "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
    "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "\n",
    "# whole dataset loading and dataset splitting \n",
    "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "\n",
    "# generate the peptide embeddings\n",
    "sequence_list = dataset['sequence'] \n",
    "embeddings_results = pd.DataFrame()\n",
    "for seq in sequence_list:\n",
    "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
    "    tuple_sequence = tuple([seq,seq])\n",
    "    peptide_sequence_list = []\n",
    "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
    "    # employ ESM model for converting and save the converted data in csv format\n",
    "    one_seq_embeddings = esm_embeddings_1280(model, alphabet, peptide_sequence_list)\n",
    "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
    "embeddings_results.to_csv('whole_sample_dataset_esm2_t33_650M_UR50D_unified_1280_dimension.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ex7esuYyY0vU"
   },
   "source": [
    "### 2560 feature dimension embedding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRUBeL3uaG0a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import esm\n",
    "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
    "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
    "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
    "model, alphabet = esm.pretrained.esm2_t36_3B_UR50D()\n",
    "\n",
    "\n",
    "# whole dataset loading and dataset splitting \n",
    "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "a=0\n",
    "# generate the peptide embeddings\n",
    "sequence_list = dataset['sequence'] \n",
    "embeddings_results = pd.DataFrame()\n",
    "for seq in sequence_list:\n",
    "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
    "    tuple_sequence = tuple([seq,seq])\n",
    "    peptide_sequence_list = []\n",
    "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
    "    # employ ESM model for converting and save the converted data in csv format\n",
    "    one_seq_embeddings = esm_embeddings_2560(model, alphabet, peptide_sequence_list)\n",
    "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
    "    a=a+1\n",
    "    print(a)\n",
    "embeddings_results.to_csv('whole_sample_dataset_esm2_t36_3B_UR50D_unified_2560_dimension.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSCXCjv8gYIr"
   },
   "source": [
    "## Generate the models that emploed at our webserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZBMpsS3_Ymu"
   },
   "source": [
    "### 320 feature dimension embedding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sguq72SHkfOL",
    "outputId": "c0c09c2e-af49-45c9-96c9-0f475d5ab742"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3212/3212 [==============================] - 5s 2ms/step\n",
      "[0.9439601494396015, 0.9439601494396015] [0.9423791821561338] [0.9455569461827285] [0.8879258003657405] [0.9829381381744644]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import esm\n",
    "\n",
    "# loading the y dataset for model development \n",
    "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "y = dataset['label']\n",
    "y = np.array(y) # transformed as np.array for CNN model\n",
    "\n",
    "# read the peptide embeddings\n",
    "X_data_name = 'whole_sample_dataset_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
    "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
    "X = np.array(X_data)\n",
    "\n",
    "# split dataset as training and test dataset as ratio of 8:2\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_whole, X_test, y_train_whole, y_test = train_test_split( X, y, test_size=0.2, random_state=123)\n",
    "X_train, X_val, y_train, y_val = train_test_split( X_train_whole, y_train_whole, test_size=0.2, random_state=123)\n",
    "\n",
    "# normalize the X data range\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train) # normalize X to 0-1 range\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "ACC_collecton = []\n",
    "BACC_collecton = []\n",
    "Sn_collecton = []\n",
    "Sp_collecton = []\n",
    "MCC_collecton = []\n",
    "AUC_collecton = []\n",
    "CNN_channel = [64]\n",
    "dense_node = [8192]\n",
    "kernel_size = [12]\n",
    "stride_size = [2]\n",
    "for i in range(len(CNN_channel)):\n",
    "  for j in range(len(dense_node)):\n",
    "    for k in range(len(kernel_size)):\n",
    "      for l in range(len(stride_size)):\n",
    "        inputShape=(320,1) # input feature size \n",
    "        input = Input(inputShape)\n",
    "        x = Conv1D(CNN_channel[i],(kernel_size[k]),strides = (stride_size[l]),name='layer_conv2',padding='same')(input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
    "        x = Dropout(0.15)(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(dense_node[j],activation = 'relu',name='fc1')(x)\n",
    "        x = Dropout(0.15)(x)\n",
    "        x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
    "        model = Model(inputs = input,outputs = x,name='Predict')\n",
    "        # define SGD optimizer\n",
    "        momentum = 0.5\n",
    "        sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
    "        # compile the model\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
    "        # learning deccay setting\n",
    "        import math\n",
    "        def step_decay(epoch): # gradually decrease the learning rate \n",
    "            initial_lrate=0.1\n",
    "            drop=0.6\n",
    "            epochs_drop = 3.0\n",
    "            lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
    "                  math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
    "            return lrate\n",
    "        lr = LearningRateScheduler(step_decay)\n",
    "        # early stop setting\n",
    "        early_stop = EarlyStopping(monitor='val_accuracy', patience = 20,verbose=0,restore_best_weights = True)\n",
    "        # set checkpoint and save the best model\n",
    "        mc = ModelCheckpoint('best_model_grid_320_server.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
    "        # summary the callbacks_list\n",
    "        callbacks_list = [ lr , early_stop, mc]\n",
    "        model_history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200,callbacks=callbacks_list,batch_size = 32, verbose=0)\n",
    "        # load the save best model\n",
    "        saved_model = load_model('best_model_grid_320_server.h5')\n",
    "        # result collection list\n",
    "        # confusion matrix \n",
    "        predicted_class= []\n",
    "        predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
    "        for p in range(predicted_protability.shape[0]):\n",
    "          index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
    "          predicted_class.append(index)\n",
    "        predicted_class = np.array(predicted_class)\n",
    "        y_true = y_test    \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import math\n",
    "        # np.ravel() return a flatten 1D array\n",
    "        TN, FP, FN, TP = confusion_matrix(y_true, predicted_class).ravel()\n",
    "        ACC = (TP+TN)/(TP+TN+FP+FN)\n",
    "        ACC_collecton.append(ACC)\n",
    "        ACC_collecton.append(ACC)\n",
    "        Sn_collecton.append(TP/(TP+FN))\n",
    "        Sp_collecton.append(TN/(TN+FP))\n",
    "        MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
    "        MCC_collecton.append(MCC)\n",
    "        BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
    "        AUC_collecton.append(AUC)\n",
    "        del model\n",
    "        import torch\n",
    "        import gc\n",
    "        torch.cuda.memory_reserved()\n",
    "        gc.collect()\n",
    "\n",
    "print(ACC_collecton, Sn_collecton, Sp_collecton, MCC_collecton, AUC_collecton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjpMrRGAjJwz"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "scaler_filename = \"320_dim.joblib\"\n",
    "joblib.dump(scaler, scaler_filename) \n",
    "\n",
    "scaler = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn_ddIHq_Ymv"
   },
   "source": [
    "### 480 feature dimension embedding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6YpKLSZkUo6",
    "outputId": "aed9b1e0-fa1d-4199-ecb5-2f90d0e22bd6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3212/3212 [==============================] - 5s 1ms/step\n",
      "0.9452054794520548\n",
      "[0.9452054794520548, 0.9452054794520548] [0.9403444034440345] [0.9501891551071879] [0.8904651923037846] [0.9852052859145667]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import esm\n",
    "\n",
    "# loading the y dataset for model development \n",
    "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "y = dataset['label']\n",
    "y = np.array(y) # transformed as np.array for CNN model\n",
    "\n",
    "# read the peptide embeddings\n",
    "X_data_name = 'whole_sample_dataset_esm2_t12_35M_UR50D_unified_480_dimension.csv'\n",
    "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
    "X = np.array(X_data)\n",
    "\n",
    "# split dataset as training and test dataset as ratio of 8:2\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_whole, X_test, y_train_whole, y_test = train_test_split( X, y, test_size=0.2, random_state=123)\n",
    "X_train, X_val, y_train, y_val = train_test_split( X_train_whole, y_train_whole, test_size=0.2, random_state=123)\n",
    "\n",
    "# normalize the X data range\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train) # normalize X to 0-1 range\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "ACC_collecton = []\n",
    "BACC_collecton = []\n",
    "Sn_collecton = []\n",
    "Sp_collecton = []\n",
    "MCC_collecton = []\n",
    "AUC_collecton = []\n",
    "\n",
    "CNN_channel = [64]\n",
    "dense_node = [2048]\n",
    "kernel_size = [12]\n",
    "stride_size = [4]\n",
    "for i in range(len(CNN_channel)):\n",
    "  for j in range(len(dense_node)):\n",
    "    for k in range(len(kernel_size)):\n",
    "      for l in range(len(stride_size)):\n",
    "        inputShape=(480,1) # input feature size \n",
    "        input = Input(inputShape)\n",
    "        x = Conv1D(CNN_channel[i],(kernel_size[k]),strides = (stride_size[l]),name='layer_conv2',padding='same')(input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
    "        x = Dropout(0.15)(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(dense_node[j],activation = 'relu',name='fc1')(x)\n",
    "        x = Dropout(0.15)(x)\n",
    "        x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
    "        model = Model(inputs = input,outputs = x,name='Predict')\n",
    "        # define SGD optimizer\n",
    "        momentum = 0.5\n",
    "        sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
    "        # compile the model\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
    "        # learning deccay setting\n",
    "        import math\n",
    "        def step_decay(epoch): # gradually decrease the learning rate \n",
    "            initial_lrate=0.1\n",
    "            drop=0.6\n",
    "            epochs_drop = 3.0\n",
    "            lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
    "                  math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
    "            return lrate\n",
    "        lr = LearningRateScheduler(step_decay)\n",
    "        # early stop setting\n",
    "        early_stop = EarlyStopping(monitor='val_accuracy', patience = 20,verbose=0,restore_best_weights = True)\n",
    "        # set checkpoint and save the best model\n",
    "        mc = ModelCheckpoint('best_model_grid_480.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
    "        # summary the callbacks_list\n",
    "        callbacks_list = [ lr , early_stop, mc]\n",
    "        model_history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200,callbacks=callbacks_list,batch_size = 32, verbose=0)\n",
    "        # load the save best model\n",
    "        saved_model = load_model('best_model_grid_480.h5')\n",
    "        # result collection list\n",
    "        # confusion matrix \n",
    "        predicted_class= []\n",
    "        predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
    "        for p in range(predicted_protability.shape[0]):\n",
    "          index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
    "          predicted_class.append(index)\n",
    "        predicted_class = np.array(predicted_class)\n",
    "        y_true = y_test    \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import math\n",
    "        # np.ravel() return a flatten 1D array\n",
    "        TN, FP, FN, TP = confusion_matrix(y_true, predicted_class).ravel()\n",
    "        ACC = (TP+TN)/(TP+TN+FP+FN)\n",
    "        ACC_collecton.append(ACC)\n",
    "        ACC_collecton.append(ACC)\n",
    "        Sn_collecton.append(TP/(TP+FN))\n",
    "        Sp_collecton.append(TN/(TN+FP))\n",
    "        MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
    "        MCC_collecton.append(MCC)\n",
    "        BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
    "        AUC_collecton.append(AUC)\n",
    "        print(ACC)\n",
    "        del model\n",
    "        import torch\n",
    "        import gc\n",
    "        torch.cuda.memory_reserved()\n",
    "        gc.collect()\n",
    "print(ACC_collecton, Sn_collecton, Sp_collecton, MCC_collecton, AUC_collecton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjP8a3hSl6vo"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "scaler_filename = \"480_dim.joblib\"\n",
    "joblib.dump(scaler, scaler_filename) \n",
    "\n",
    "scaler = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXbl2M2w_Ymw"
   },
   "source": [
    "### 640 feature dimension embedding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8J_Da_KbkPgS",
    "outputId": "8e3e9bba-0096-4d2d-9503-782f533fe953"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3212/3212 [==============================] - 5s 1ms/step\n",
      "0.951120797011208\n",
      "[0.951120797011208, 0.951120797011208] [0.9606349206349206] [0.9419670128283445] [0.9024344669091698] [0.9873359589211053]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import esm\n",
    "\n",
    "# loading the y dataset for model development \n",
    "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "y = dataset['label']\n",
    "y = np.array(y) # transformed as np.array for CNN model\n",
    "\n",
    "# read the peptide embeddings\n",
    "X_data_name = 'whole_sample_dataset_esm2_t30_150M_UR50D_unified_640_dimension.csv'\n",
    "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
    "X = np.array(X_data)\n",
    "\n",
    "# split dataset as training and test dataset as ratio of 8:2\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_whole, X_test, y_train_whole, y_test = train_test_split( X, y, test_size=0.2, random_state=123)\n",
    "X_train, X_val, y_train, y_val = train_test_split( X_train_whole, y_train_whole, test_size=0.2, random_state=123)\n",
    "\n",
    "# normalize the X data range\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train) # normalize X to 0-1 range\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "ACC_collecton = []\n",
    "BACC_collecton = []\n",
    "Sn_collecton = []\n",
    "Sp_collecton = []\n",
    "MCC_collecton = []\n",
    "AUC_collecton = []\n",
    "\n",
    "CNN_channel = [32]\n",
    "dense_node = [4096]\n",
    "kernel_size = [9]\n",
    "stride_size = [2]\n",
    "for i in range(len(CNN_channel)):\n",
    "  for j in range(len(dense_node)):\n",
    "    for k in range(len(kernel_size)):\n",
    "      for l in range(len(stride_size)):\n",
    "        inputShape=(640,1) # input feature size \n",
    "        input = Input(inputShape)\n",
    "        x = Conv1D(CNN_channel[i],(kernel_size[k]),strides = (stride_size[l]),name='layer_conv2',padding='same')(input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
    "        x = Dropout(0.15)(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(dense_node[j],activation = 'relu',name='fc1')(x)\n",
    "        x = Dropout(0.15)(x)\n",
    "        x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
    "        model = Model(inputs = input,outputs = x,name='Predict')\n",
    "        # define SGD optimizer\n",
    "        momentum = 0.5\n",
    "        sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
    "        # compile the model\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
    "        # learning deccay setting\n",
    "        import math\n",
    "        def step_decay(epoch): # gradually decrease the learning rate \n",
    "            initial_lrate=0.1\n",
    "            drop=0.6\n",
    "            epochs_drop = 3.0\n",
    "            lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
    "                  math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
    "            return lrate\n",
    "        lr = LearningRateScheduler(step_decay)\n",
    "        # early stop setting\n",
    "        early_stop = EarlyStopping(monitor='val_accuracy', patience = 20,verbose=0,restore_best_weights = True)\n",
    "        # set checkpoint and save the best model\n",
    "        mc = ModelCheckpoint('best_model_grid_640.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
    "        # summary the callbacks_list\n",
    "        callbacks_list = [ lr , early_stop, mc]\n",
    "        model_history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200,callbacks=callbacks_list,batch_size = 32, verbose=0)\n",
    "        # load the save best model\n",
    "        saved_model = load_model('best_model_grid_640.h5')\n",
    "        # result collection list\n",
    "        # confusion matrix \n",
    "        predicted_class= []\n",
    "        predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
    "        for p in range(predicted_protability.shape[0]):\n",
    "          index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
    "          predicted_class.append(index)\n",
    "        predicted_class = np.array(predicted_class)\n",
    "        y_true = y_test    \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import math\n",
    "        # np.ravel() return a flatten 1D array\n",
    "        TN, FP, FN, TP = confusion_matrix(y_true, predicted_class).ravel() \n",
    "        ACC = (TP+TN)/(TP+TN+FP+FN)\n",
    "        ACC_collecton.append(ACC)\n",
    "        ACC_collecton.append(ACC)\n",
    "        Sn_collecton.append(TP/(TP+FN))\n",
    "        Sp_collecton.append(TN/(TN+FP))\n",
    "        MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
    "        MCC_collecton.append(MCC)\n",
    "        BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
    "        AUC_collecton.append(AUC)\n",
    "        print(ACC)\n",
    "        del model\n",
    "        import torch\n",
    "        import gc\n",
    "        torch.cuda.memory_reserved()\n",
    "        gc.collect()\n",
    "print(ACC_collecton, Sn_collecton, Sp_collecton, MCC_collecton, AUC_collecton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hzLmLukmAEV"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "scaler_filename = \"640_dim.joblib\"\n",
    "joblib.dump(scaler, scaler_filename) \n",
    "\n",
    "scaler = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riQQE4SS_Ymy"
   },
   "source": [
    "### 1280 feature dimension embedding test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LaLj2PIgckpH",
    "outputId": "5ededd3b-4f97-4c1e-a48d-87e20d65f4ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3212/3212 [==============================] - 5s 1ms/step\n",
      "0.9539227895392279\n",
      "[0.9539227895392279, 0.9539227895392279] [0.9467564259485924] [0.9613434727503168] [0.9079625765269058] [0.9884217472487935]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import esm\n",
    "\n",
    "# loading the y dataset for model development \n",
    "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "y = dataset['label']\n",
    "y = np.array(y) # transformed as np.array for CNN model\n",
    "\n",
    "# read the peptide embeddings\n",
    "X_data_name = 'whole_sample_dataset_esm2_t33_650M_UR50D_unified_1280_dimension.csv'\n",
    "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
    "X = np.array(X_data)\n",
    "\n",
    "# split dataset as training and test dataset as ratio of 8:2\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_whole, X_test, y_train_whole, y_test = train_test_split( X, y, test_size=0.2, random_state=123)\n",
    "X_train, X_val, y_train, y_val = train_test_split( X_train_whole, y_train_whole, test_size=0.2, random_state=123)\n",
    "\n",
    "# normalize the X data range\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train) # normalize X to 0-1 range\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "ACC_collecton = []\n",
    "BACC_collecton = []\n",
    "Sn_collecton = []\n",
    "Sp_collecton = []\n",
    "MCC_collecton = []\n",
    "AUC_collecton = []\n",
    "\n",
    "CNN_channel = [32]\n",
    "dense_node = [1024]\n",
    "kernel_size = [9]\n",
    "stride_size = [8]\n",
    "for i in range(len(CNN_channel)):\n",
    "  for j in range(len(dense_node)):\n",
    "    for k in range(len(kernel_size)):\n",
    "      for l in range(len(stride_size)):\n",
    "        inputShape=(1280,1) # input feature size \n",
    "        input = Input(inputShape)\n",
    "        x = Conv1D(CNN_channel[i],(kernel_size[k]),strides = (stride_size[l]),name='layer_conv2',padding='same')(input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
    "        x = Dropout(0.15)(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(dense_node[j],activation = 'relu',name='fc1')(x)\n",
    "        x = Dropout(0.15)(x)\n",
    "        x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
    "        model = Model(inputs = input,outputs = x,name='Predict')\n",
    "        # define SGD optimizer\n",
    "        momentum = 0.5\n",
    "        sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
    "        # compile the model\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
    "        # learning deccay setting\n",
    "        import math\n",
    "        def step_decay(epoch): # gradually decrease the learning rate \n",
    "            initial_lrate=0.1\n",
    "            drop=0.6\n",
    "            epochs_drop = 3.0\n",
    "            lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
    "                  math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
    "            return lrate\n",
    "        lr = LearningRateScheduler(step_decay)\n",
    "        # early stop setting\n",
    "        early_stop = EarlyStopping(monitor='val_accuracy', patience = 20,verbose=0,restore_best_weights = True)\n",
    "        # set checkpoint and save the best model\n",
    "        mc = ModelCheckpoint('best_model_grid_1280_server.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
    "        # summary the callbacks_list\n",
    "        callbacks_list = [ lr , early_stop, mc]\n",
    "        model_history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200,callbacks=callbacks_list,batch_size = 32, verbose=0)\n",
    "        # load the save best model\n",
    "        saved_model = load_model('best_model_grid_1280_server.h5')\n",
    "        # result collection list\n",
    "        # confusion matrix \n",
    "        predicted_class= []\n",
    "        predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
    "        for p in range(predicted_protability.shape[0]):\n",
    "          index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
    "          predicted_class.append(index)\n",
    "        predicted_class = np.array(predicted_class)\n",
    "        y_true = y_test    \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import math\n",
    "        # np.ravel() return a flatten 1D array\n",
    "        TN, FP, FN, TP = confusion_matrix(y_true, predicted_class).ravel() \n",
    "        ACC = (TP+TN)/(TP+TN+FP+FN)\n",
    "        ACC_collecton.append(ACC)\n",
    "        ACC_collecton.append(ACC)\n",
    "        Sn_collecton.append(TP/(TP+FN))\n",
    "        Sp_collecton.append(TN/(TN+FP))\n",
    "        MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
    "        MCC_collecton.append(MCC)\n",
    "        BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
    "        AUC_collecton.append(AUC)\n",
    "        print(ACC)\n",
    "        del model\n",
    "        import torch\n",
    "        import gc\n",
    "        torch.cuda.memory_reserved()\n",
    "        gc.collect()\n",
    "print(ACC_collecton, Sn_collecton, Sp_collecton, MCC_collecton, AUC_collecton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2FPuSfvVKKI"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "scaler_filename = \"1280_dim.joblib\"\n",
    "joblib.dump(scaler, scaler_filename) \n",
    "\n",
    "scaler = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8q0KSsR_Ymz"
   },
   "source": [
    "### 2560 feature dimension embedding test\n",
    "Due to the enourmous computational needs and also the limitted performance improivement, we do not test the 128 and 256 fileter size with 8196 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xk1v0R7hCRFI",
    "outputId": "974941a5-835f-433c-9516-0c7b873867ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3212/3212 [==============================] - 7s 2ms/step\n",
      "0.9579701120797012\n",
      "[0.9579701120797012, 0.9579701120797012] [0.9594510293200249] [0.9564947172156619] [0.9159448587266507] [0.9910083699333756]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import esm\n",
    "\n",
    "# loading the y dataset for model development \n",
    "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "y = dataset['label']\n",
    "y = np.array(y) # transformed as np.array for CNN model\n",
    "\n",
    "# read the peptide embeddings\n",
    "X_data_name = 'whole_sample_dataset_esm2_t36_3B_UR50D_unified_2560_dimension.csv'\n",
    "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
    "X = np.array(X_data)\n",
    "\n",
    "# split dataset as training and test dataset as ratio of 8:2\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_whole, X_test, y_train_whole, y_test = train_test_split( X, y, test_size=0.2, random_state=123)\n",
    "X_train, X_val, y_train, y_val = train_test_split( X_train_whole, y_train_whole, test_size=0.2, random_state=123)\n",
    "\n",
    "# normalize the X data range\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train) # normalize X to 0-1 range\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "ACC_collecton = []\n",
    "BACC_collecton = []\n",
    "Sn_collecton = []\n",
    "Sp_collecton = []\n",
    "MCC_collecton = []\n",
    "AUC_collecton = []\n",
    "\n",
    "CNN_channel = [64] # \n",
    "dense_node = [4096]\n",
    "kernel_size = [6]\n",
    "stride_size = [4]\n",
    "for i in range(len(CNN_channel)):\n",
    "  for j in range(len(dense_node)):\n",
    "    for k in range(len(kernel_size)):\n",
    "      for l in range(len(stride_size)):\n",
    "        inputShape=(2560,1) # input feature size \n",
    "        input = Input(inputShape)\n",
    "        x = Conv1D(CNN_channel[i],(kernel_size[k]),strides = (stride_size[l]),name='layer_conv2',padding='same')(input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
    "        x = Dropout(0.15)(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(dense_node[j],activation = 'relu',name='fc1')(x)\n",
    "        x = Dropout(0.15)(x)\n",
    "        x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
    "        model = Model(inputs = input,outputs = x,name='Predict')\n",
    "        # define SGD optimizer\n",
    "        momentum = 0.5\n",
    "        sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
    "        # compile the model\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
    "        # learning deccay setting\n",
    "        import math\n",
    "        def step_decay(epoch): # gradually decrease the learning rate \n",
    "            initial_lrate=0.1\n",
    "            drop=0.6\n",
    "            epochs_drop = 3.0\n",
    "            lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
    "                  math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
    "            return lrate\n",
    "        lr = LearningRateScheduler(step_decay)\n",
    "        # early stop setting\n",
    "        early_stop = EarlyStopping(monitor='val_accuracy', patience = 20,verbose=0,restore_best_weights = True)\n",
    "        # set checkpoint and save the best model\n",
    "        mc = ModelCheckpoint('best_model_grid_2560_server.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
    "        # summary the callbacks_list\n",
    "        callbacks_list = [ lr , early_stop, mc]\n",
    "        model_history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200,callbacks=callbacks_list,batch_size = 32, verbose=0)\n",
    "        # load the save best model\n",
    "        saved_model = load_model('best_model_grid_2560_server.h5')\n",
    "        # result collection list\n",
    "        # confusion matrix \n",
    "        predicted_class= []\n",
    "        predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
    "        for p in range(predicted_protability.shape[0]):\n",
    "          index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
    "          predicted_class.append(index)\n",
    "        predicted_class = np.array(predicted_class)\n",
    "        y_true = y_test    \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import math\n",
    "        # np.ravel() return a flatten 1D array\n",
    "        TN, FP, FN, TP = confusion_matrix(y_true, predicted_class).ravel() \n",
    "        ACC = (TP+TN)/(TP+TN+FP+FN)\n",
    "        ACC_collecton.append(ACC)\n",
    "        ACC_collecton.append(ACC)\n",
    "        Sn_collecton.append(TP/(TP+FN))\n",
    "        Sp_collecton.append(TN/(TN+FP))\n",
    "        MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
    "        MCC_collecton.append(MCC)\n",
    "        BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
    "        AUC_collecton.append(AUC)\n",
    "        print(ACC)\n",
    "        del model\n",
    "        import torch\n",
    "        import gc\n",
    "        torch.cuda.memory_reserved()\n",
    "        gc.collect()\n",
    "print(ACC_collecton, Sn_collecton, Sp_collecton, MCC_collecton, AUC_collecton)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sX2lKNQdmHJc"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "scaler_filename = \"2560_dim.joblib\"\n",
    "joblib.dump(scaler, scaler_filename) \n",
    "\n",
    "scaler = joblib.load(scaler_filename)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "95NTckuFZZzm",
    "m91cA0H5w_eY",
    "5RLR10hUsxca",
    "ztgr6bl-YGDI",
    "C3yzGDFEYfCY",
    "mFyOy-19Yvil",
    "dEdAluvxYzfJ",
    "Ex7esuYyY0vU",
    "Vn_ddIHq_Ymv",
    "HXbl2M2w_Ymw",
    "riQQE4SS_Ymy",
    "tkgD_C1RDwWh",
    "xAMCI-1F5DsR",
    "nVsxPkQe5DsT",
    "Borpedo25DsU",
    "shaxu0Gh5DsV",
    "yqp51vUi5DsY"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
