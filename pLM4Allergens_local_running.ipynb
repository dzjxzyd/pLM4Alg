{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95NTckuFZZzm"
      },
      "source": [
        "## package installation and load packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO71IBS6ZgZV"
      },
      "outputs": [],
      "source": [
        "### packages required \n",
        "!pip install fair-esm \n",
        "!pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install sklearn\n",
        "!pip install h5py\n",
        "!pip install joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3sxRX-RfzoB",
        "outputId": "feaf4f52-afc4-4e62-e3f9-59b1debe089f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU found\n"
          ]
        }
      ],
      "source": [
        "import esm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
        "from keras.layers import Dropout, AveragePooling1D, MaxPooling1D\n",
        "from keras.models import Sequential,Model, load_model\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint,LearningRateScheduler, EarlyStopping\n",
        "import keras\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name():\n",
        "    print('GPU found')\n",
        "    tf.config.experimental.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU') # set the deep learning with GPU \n",
        "else:\n",
        "    print(\"No GPU found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m91cA0H5w_eY"
      },
      "source": [
        "### peptide embeddings with differen pretrained model\n",
        "https://github.com/facebookresearch/esm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmb2VZKeZlWC"
      },
      "source": [
        "Explaination of the memeory usage of the following models\n",
        "\n",
        "sequence length > 900 \n",
        "\n",
        "2560 output dimension model might need 24 G GPU memory\n",
        "\n",
        "5129 output dimension model, (in our attempts, 40 GB GPU memory is not enough) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl7XVx5HZsHf"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_320(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != esm2_alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t6_8M_UR50D' only has 6 layers, and therefore repr_layers parameters is equal to 6\n",
        "      results = esm2(batch_tokens, repr_layers=[6], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][6].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXIH6LOsV-6v"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_480(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != esm2_alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t12_35M_UR50D' only has 12 layers, and therefore repr_layers parameters is equal to 12\n",
        "      results = esm2(batch_tokens, repr_layers=[12], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][12].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN7A3bgzhFny"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_640(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != esm2_alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t30_150M_UR50D' only has 30 layers, and therefore repr_layers parameters is equal to 30\n",
        "      results = esm2(batch_tokens, repr_layers=[30], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][30].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdQIPEe2V3lB"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_1280(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t33_650M_UR50D' only has 33 layers, and therefore repr_layers parameters is equal to 33\n",
        "      results = esm2(batch_tokens, repr_layers=[33], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][33].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmr0OfcTWN05"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_2560(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t36_3B_UR50D' only has 36 layers, and therefore repr_layers parameters is equal to 36\n",
        "      results = esm2(batch_tokens, repr_layers=[36], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][36].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kjrdZscWULt"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_5120(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t48_15B_UR50D' only has 48 layers, and therefore repr_layers parameters is equal to 48\n",
        "      results = esm2(batch_tokens, repr_layers=[48], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][48].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJ-0YPyiQhp"
      },
      "source": [
        "### load packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTetST5AiUAh",
        "outputId": "2798cce7-4410-4fb4-db6a-0f7100bb9462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU found\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
        "from keras.layers import Dropout, AveragePooling1D, MaxPooling1D\n",
        "from keras.models import Sequential,Model, load_model\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint,LearningRateScheduler, EarlyStopping\n",
        "import keras\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name():\n",
        "    print('GPU found')\n",
        "    tf.config.experimental.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU') # set the deep learning with GPU \n",
        "else:\n",
        "    print(\"No GPU found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbT7lHdkFrSQ"
      },
      "source": [
        "## Load your sample and our developed model for local running"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# collect the output\n",
        "def assign_activity(predicted_class):\n",
        "    import collections\n",
        "    out_put = []\n",
        "    for i in range(len(predicted_class)):\n",
        "        if predicted_class[i] == 0:\n",
        "            # out_put[int_features[i]].append(1)\n",
        "            out_put.append('Allergen')\n",
        "        else:\n",
        "            # out_put[int_features[i]].append(2)\n",
        "            out_put.append('Non-allergen')\n",
        "    return out_put"
      ],
      "metadata": {
        "id": "fF233nCgI6l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7CEN1LRFrSR"
      },
      "source": [
        "### 320 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd-mIwcLFrSR"
      },
      "outputs": [],
      "source": [
        "# embedding your sample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
        "\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings_320(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "scaler = joblib.load('best_model_grid_320_server.joblib')\n",
        "normalized_embeddings_results = scaler.transform(embeddings_results)\n",
        "\n",
        "from keras.models import load_model\n",
        "# load the save best model\n",
        "saved_model = load_model('best_model_grid_320_server.h5')\n",
        "# result collection list\n",
        "# confusion matrix \n",
        "predicted_class= []\n",
        "predicted_protability = saved_model.predict(normalized_embeddings_results,batch_size=1)\n",
        "for p in range(predicted_protability.shape[0]):\n",
        "  index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "  predicted_class.append(index)\n",
        "predicted_class = np.array(predicted_class)\n",
        "\n",
        "predicted_class = assign_activity(predicted_class)  # transform results (0 and 1) into 'active' and 'non-active'\n",
        "print(predicted_class)\n",
        "report = {\"sequence\": sequence_list, \"activity\": predicted_class}\n",
        "report_df = pd.DataFrame(report)\n",
        "report_df.to_excel(\"320_report.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yRcLLMBGQkJ",
        "outputId": "25a0f9ea-a86f-452c-f463-2fa00b8ea4a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 19ms/step\n",
            "['Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxnKl3eMFrSR"
      },
      "source": [
        "### 480 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxV-FHEHFrSR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
        "\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings_480(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('whole_sample_dataset_esm2_t12_35M_UR50D_unified_480_dimension.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "scaler = joblib.load('best_model_grid_480_server.joblib')\n",
        "normalized_embeddings_results = scaler.transform(embeddings_results)\n",
        "\n",
        "from keras.models import load_model\n",
        "# load the save best model\n",
        "saved_model = load_model('best_model_grid_480_server.h5')\n",
        "# result collection list\n",
        "# confusion matrix \n",
        "predicted_class= []\n",
        "predicted_protability = saved_model.predict(normalized_embeddings_results,batch_size=1)\n",
        "for p in range(predicted_protability.shape[0]):\n",
        "  index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "  predicted_class.append(index)\n",
        "predicted_class = np.array(predicted_class)\n",
        "\n",
        "predicted_class = assign_activity(predicted_class)  # transform results (0 and 1) into 'active' and 'non-active'\n",
        "print(predicted_class)\n",
        "report = {\"sequence\": sequence_list, \"activity\": predicted_class}\n",
        "report_df = pd.DataFrame(report)\n",
        "report_df.to_excel(\"480_report.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHhgkdzWJkWv",
        "outputId": "75e2b821-2a0e-440d-c999-32b01928537b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 1s 13ms/step\n",
            "['Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVNy8QesFrSR"
      },
      "source": [
        "### 640 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB3vwdRUFrSS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
        "\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings_640(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('whole_sample_dataset_esm2_t30_150M_UR50D_unified_640_dimension.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "scaler = joblib.load('best_model_grid_640_server.joblib')\n",
        "normalized_embeddings_results = scaler.transform(embeddings_results)\n",
        "\n",
        "from keras.models import load_model\n",
        "# load the save best model\n",
        "saved_model = load_model('best_model_grid_640_server.h5')\n",
        "# result collection list\n",
        "# confusion matrix \n",
        "predicted_class= []\n",
        "predicted_protability = saved_model.predict(normalized_embeddings_results,batch_size=1)\n",
        "for p in range(predicted_protability.shape[0]):\n",
        "  index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "  predicted_class.append(index)\n",
        "predicted_class = np.array(predicted_class)\n",
        "\n",
        "predicted_class = assign_activity(predicted_class)  # transform results (0 and 1) into 'active' and 'non-active'\n",
        "print(predicted_class)\n",
        "report = {\"sequence\": sequence_list, \"activity\": predicted_class}\n",
        "report_df = pd.DataFrame(report)\n",
        "report_df.to_excel(\"640_report.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vji2LLmJoPY",
        "outputId": "41ec6f4a-b4a6-4064-ede5-7636581bbf9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 15ms/step\n",
            "['Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m--JgG27FrSS"
      },
      "source": [
        "### 1280 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M1bFON3FrSa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings_1280(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('whole_sample_dataset_esm2_t33_650M_UR50D_unified_1280_dimension.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "scaler = joblib.load('best_model_grid_1280_server.joblib')\n",
        "normalized_embeddings_results = scaler.transform(embeddings_results)\n",
        "\n",
        "from keras.models import load_model\n",
        "# load the save best model\n",
        "saved_model = load_model('best_model_grid_1280_server.h5')\n",
        "# result collection list\n",
        "# confusion matrix \n",
        "predicted_class= []\n",
        "predicted_protability = saved_model.predict(normalized_embeddings_results,batch_size=1)\n",
        "for p in range(predicted_protability.shape[0]):\n",
        "  index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "  predicted_class.append(index)\n",
        "predicted_class = np.array(predicted_class)\n",
        "\n",
        "predicted_class = assign_activity(predicted_class)  # transform results (0 and 1) into 'active' and 'non-active'\n",
        "print(predicted_class)\n",
        "report = {\"sequence\": sequence_list, \"activity\": predicted_class}\n",
        "report_df = pd.DataFrame(report)\n",
        "report_df.to_excel(\"1280_report.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2wsEqwaJsHy",
        "outputId": "5a26f538-1dbe-4fc1-dfd9-499011992f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 4ms/step\n",
            "['Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen', 'Non-allergen']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S65mG00wFrSb"
      },
      "source": [
        "### 2560 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVNiI4_aFrSb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t36_3B_UR50D()\n",
        "\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "a=0\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings_2560(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "    a=a+1\n",
        "    print(a)\n",
        "embeddings_results.to_csv('whole_sample_dataset_esm2_t36_3B_UR50D_unified_2560_dimension.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "scaler = joblib.load('best_model_grid_2560_server.joblib')\n",
        "normalized_embeddings_results = scaler.transform(embeddings_results)\n",
        "\n",
        "from keras.models import load_model\n",
        "# load the save best model\n",
        "saved_model = load_model('best_model_grid_2560_server.h5')\n",
        "# result collection list\n",
        "# confusion matrix \n",
        "predicted_class= []\n",
        "predicted_protability = saved_model.predict(normalized_embeddings_results,batch_size=1)\n",
        "for p in range(predicted_protability.shape[0]):\n",
        "  index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "  predicted_class.append(index)\n",
        "predicted_class = np.array(predicted_class)\n",
        "\n",
        "predicted_class = assign_activity(predicted_class)  # transform results (0 and 1) into 'active' and 'non-active'\n",
        "print(predicted_class)\n",
        "report = {\"sequence\": sequence_list, \"activity\": predicted_class}\n",
        "report_df = pd.DataFrame(report)\n",
        "report_df.to_excel(\"2560_report.xlsx\")\n"
      ],
      "metadata": {
        "id": "AME0_NZgKAp6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}