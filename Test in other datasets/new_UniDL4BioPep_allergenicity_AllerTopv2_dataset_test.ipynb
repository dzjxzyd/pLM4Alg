{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95NTckuFZZzm"
      },
      "source": [
        "### requirements for the following codings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO71IBS6ZgZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3db64ea5-3975-43f5-e334-3fc05151b3ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post5.tar.gz (3.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post5-py3-none-any.whl size=2950 sha256=558bb419ac14efe904eb927f49887f2654a90eb36419bbbf954c70c95314e8a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/1f/8d/4f812c590e074c1e928f5cec67bf5053b71f38e2648739403a\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.22.4)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.81\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "### packages required \n",
        "!pip install fair-esm \n",
        "!pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install sklearn\n",
        "!pip install biopython\n",
        "!pip install h5py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m91cA0H5w_eY"
      },
      "source": [
        "### peptide embeddings with differen pretrained model\n",
        "https://github.com/facebookresearch/esm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl7XVx5HZsHf"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_320(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t6_8M_UR50D' only has 6 layers, and therefore repr_layers parameters is equal to 6\n",
        "      results = esm2(batch_tokens, repr_layers=[6], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][6].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXIH6LOsV-6v"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_480(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t12_35M_UR50D' only has 12 layers, and therefore repr_layers parameters is equal to 12\n",
        "      results = esm2(batch_tokens, repr_layers=[12], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][12].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN7A3bgzhFny"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_640(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t30_150M_UR50D' only has 30 layers, and therefore repr_layers parameters is equal to 30\n",
        "      results = esm2(batch_tokens, repr_layers=[30], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][30].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdQIPEe2V3lB"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_1280(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t33_650M_UR50D' only has 33 layers, and therefore repr_layers parameters is equal to 33\n",
        "      results = esm2(batch_tokens, repr_layers=[33], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][33].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmr0OfcTWN05"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_2560(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t36_3B_UR50D' only has 36 layers, and therefore repr_layers parameters is equal to 36\n",
        "      results = esm2(batch_tokens, repr_layers=[36], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][36].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kjrdZscWULt"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_5120(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t48_15B_UR50D' only has 48 layers, and therefore repr_layers parameters is equal to 48\n",
        "      results = esm2(batch_tokens, repr_layers=[48], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][48].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJ-0YPyiQhp"
      },
      "source": [
        "#### load packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTetST5AiUAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb0e1cd-5201-4e06-b63f-b711f060005c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU found\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
        "from keras.layers import Dropout, AveragePooling1D, MaxPooling1D\n",
        "from keras.models import Sequential,Model, load_model\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint,LearningRateScheduler, EarlyStopping\n",
        "import keras\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name():\n",
        "    print('GPU found')\n",
        "    tf.config.experimental.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU') # set the deep learning with GPU \n",
        "else:\n",
        "    print(\"No GPU found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66lgDXw4-fE"
      },
      "source": [
        "## 10 times repeated experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAMCI-1F5DsR"
      },
      "source": [
        "### 320 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2Ugzfs9MMa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffb48b8e-f086-4a37-82aa-d76fc52646dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1 1 1\n"
          ]
        }
      ],
      "source": [
        "CNN_channel = [64] # set as the best parameters\n",
        "dense_node = [8192]\n",
        "kernel_size = [12]\n",
        "stride_size = [2]\n",
        "print(len(CNN_channel),len(dense_node), len(kernel_size),len(stride_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NJab0Rm5DsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22cc2b7b-f64d-4267-96fb-088f8c5356a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t6_8M_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t6_8M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t6_8M_UR50D-contact-regression.pt\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 2s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 2s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 2s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 2s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 2s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 2s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 2s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 2s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 2s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 2s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('AllerTOPv2.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings_320(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('whole_sample_dataset_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n",
        "\n",
        "# loading the y dataset for model development \n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data_name = 'whole_sample_dataset_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
        "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "ACC_repeat_col=[]\n",
        "BACC_repeat_col = []\n",
        "Sn_repeat_col= []\n",
        "Sp_repeat_col=[]\n",
        "MCC_repeat_col=[]\n",
        "AUC_repeat_col=[]\n",
        "\n",
        "for i in range(len(CNN_channel)):\n",
        "  # split dataset as training and test dataset as ratio of 8:2\n",
        "  ACC_collecton = []\n",
        "  BACC_collecton = []\n",
        "  Sn_collecton = []\n",
        "  Sp_collecton = []\n",
        "  MCC_collecton = []\n",
        "  AUC_collecton = []\n",
        "  for random_num in range(10):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=random_num)\n",
        "\n",
        "    # normalize the X data range\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    inputShape=(320,1) # input feature size \n",
        "    input = Input(inputShape)\n",
        "    x = Conv1D(CNN_channel[i],(kernel_size[i]),strides = (stride_size[i]),name='layer_conv2',padding='same')(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(dense_node[i],activation = 'relu',name='fc1')(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "    model = Model(inputs = input,outputs = x,name='Predict')\n",
        "    # define SGD optimizer\n",
        "    momentum = 0.5\n",
        "    sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "    # compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "    # learning deccay setting\n",
        "    import math\n",
        "    def step_decay(epoch): # gradually decrease the learning rate \n",
        "        initial_lrate=0.1\n",
        "        drop=0.6\n",
        "        epochs_drop = 3.0\n",
        "        lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "              math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "        return lrate\n",
        "    lr = LearningRateScheduler(step_decay)\n",
        "    # early stop setting\n",
        "    early_stop = EarlyStopping(monitor='val_accuracy', patience = 40,verbose=0,restore_best_weights = True)\n",
        "    # set checkpoint and save the best model\n",
        "    mc = ModelCheckpoint('best_modelbest_320_10times.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
        "    # summary the callbacks_list\n",
        "    callbacks_list = [ lr , early_stop, mc]\n",
        "    model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 16, verbose=0)\n",
        "    # load the save best model\n",
        "    saved_model = load_model('best_modelbest_320_10times.h5')\n",
        "    # result collection list\n",
        "    # confusion matrix \n",
        "    predicted_class= []\n",
        "    predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "    for p in range(predicted_protability.shape[0]):\n",
        "      index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "      predicted_class.append(index)\n",
        "    predicted_class = np.array(predicted_class)\n",
        "    y_true = y_test    \n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import math\n",
        "    # np.ravel() return a flatten 1D array\n",
        "    TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "    ACC_collecton.append(ACC)\n",
        "    Sn_collecton.append(TP/(TP+FN))\n",
        "    Sp_collecton.append(TN/(TN+FP))\n",
        "    MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
        "    MCC_collecton.append(MCC)\n",
        "    BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
        "    AUC_collecton.append(AUC)\n",
        "    del model\n",
        "    import torch\n",
        "    import gc\n",
        "    torch.cuda.memory_reserved()\n",
        "    gc.collect()\n",
        "  import statistics\n",
        "  ACC_repeat_col.append(str(round(statistics.mean(ACC_collecton),3))+'±'+ str(round(statistics.stdev(ACC_collecton),3)))\n",
        "  BACC_repeat_col.append(str(round(statistics.mean(BACC_collecton),3))+'±'+str(round(statistics.stdev(BACC_collecton),3)))\n",
        "  Sn_repeat_col.append(str(round(statistics.mean(Sn_collecton),3))+'±'+str(round(statistics.stdev(Sn_collecton),3)))\n",
        "  Sp_repeat_col.append(str(round(statistics.mean(Sp_collecton),3))+'±'+str(round(statistics.stdev(Sp_collecton),3)))\n",
        "  MCC_repeat_col.append(str(round(statistics.mean(MCC_collecton),3))+'±'+str(round(statistics.stdev(MCC_collecton),3)))\n",
        "  AUC_repeat_col.append(str(round(statistics.mean(AUC_collecton),3))+'±'+str(round(statistics.stdev(AUC_collecton),3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCRm5lh5SRH7"
      },
      "outputs": [],
      "source": [
        "# combine the lists into a list of tuples\n",
        "combined_list = list(zip(ACC_repeat_col, BACC_repeat_col, Sn_repeat_col, Sp_repeat_col, MCC_repeat_col, AUC_repeat_col))\n",
        "\n",
        "# create a DataFrame from the list of tuples\n",
        "df = pd.DataFrame(combined_list, columns=['ACC_collection', 'BACC_collection', 'Sn_collection', 'Sp_collection', 'MCC_collection', 'AUC_collection'])\n",
        "\n",
        "# export the DataFrame to an Excel file\n",
        "df.to_excel('AllerStat_320.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVsxPkQe5DsT"
      },
      "source": [
        "### 480 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS3iD6_aQFO1",
        "outputId": "2794cc89-cf1a-47e9-bb5d-a979bb928ba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1 1 1\n"
          ]
        }
      ],
      "source": [
        "CNN_channel = [64] # set as the best parameters\n",
        "dense_node = [2048]\n",
        "kernel_size = [12]\n",
        "stride_size = [4]\n",
        "print(len(CNN_channel),len(dense_node), len(kernel_size),len(stride_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N54VN0CY5DsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "759de06a-5cc0-43b1-986c-1fd0acef48af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t12_35M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t12_35M_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t12_35M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t12_35M_UR50D-contact-regression.pt\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n",
            "0.921±0.007 0.921±0.006 0.928±0.014 0.914±0.021 0.842±0.013 0.973±0.004\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
        "\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('AllerTOPv2.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "\n",
        "# # generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings_480(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('whole_sample_dataset_esm2_t12_35M_UR50D_unified_480_dimension.csv')\n",
        "\n",
        "# loading the y dataset for model development \n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data_name = 'whole_sample_dataset_esm2_t12_35M_UR50D_unified_480_dimension.csv'\n",
        "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "ACC_repeat_col=[]\n",
        "BACC_repeat_col = []\n",
        "Sn_repeat_col= []\n",
        "Sp_repeat_col=[]\n",
        "MCC_repeat_col=[]\n",
        "AUC_repeat_col=[]\n",
        "for i in range(len(CNN_channel)):\n",
        "  # split dataset as training and test dataset as ratio of 8:2\n",
        "  ACC_collecton = []\n",
        "  BACC_collecton = []\n",
        "  Sn_collecton = []\n",
        "  Sp_collecton = []\n",
        "  MCC_collecton = []\n",
        "  AUC_collecton = []\n",
        "  for random_num in range(10):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=random_num)\n",
        "\n",
        "    # normalize the X data range\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    inputShape=(480,1) # input feature size \n",
        "    input = Input(inputShape)\n",
        "    x = Conv1D(CNN_channel[i],(kernel_size[i]),strides = (stride_size[i]),name='layer_conv2',padding='same')(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(dense_node[i],activation = 'relu',name='fc1')(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "    model = Model(inputs = input,outputs = x,name='Predict')\n",
        "    # define SGD optimizer\n",
        "    momentum = 0.5\n",
        "    sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "    # compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "    # learning deccay setting\n",
        "    import math\n",
        "    def step_decay(epoch): # gradually decrease the learning rate \n",
        "        initial_lrate=0.1\n",
        "        drop=0.6\n",
        "        epochs_drop = 3.0\n",
        "        lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "              math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "        return lrate\n",
        "    lr = LearningRateScheduler(step_decay)\n",
        "    # early stop setting\n",
        "    early_stop = EarlyStopping(monitor='val_accuracy', patience = 40,verbose=0,restore_best_weights = True)\n",
        "    # set checkpoint and save the best model\n",
        "    mc = ModelCheckpoint('best_modelbest_480_repeat_1.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
        "    # summary the callbacks_list\n",
        "    callbacks_list = [ lr , early_stop, mc]\n",
        "    model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 16, verbose=0)\n",
        "    # load the save best model\n",
        "    saved_model = load_model('best_modelbest_480_repeat_1.h5')\n",
        "    # result collection list\n",
        "    # confusion matrix \n",
        "    predicted_class= []\n",
        "    predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "    for p in range(predicted_protability.shape[0]):\n",
        "      index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "      predicted_class.append(index)\n",
        "    predicted_class = np.array(predicted_class)\n",
        "    y_true = y_test    \n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import math\n",
        "    # np.ravel() return a flatten 1D array\n",
        "    TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "    if ACC>0.6:\n",
        "      ACC_collecton.append(ACC)\n",
        "      Sn_collecton.append(TP/(TP+FN))\n",
        "      Sp_collecton.append(TN/(TN+FP))\n",
        "      MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
        "      MCC_collecton.append(MCC)\n",
        "      BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
        "      from sklearn.metrics import roc_auc_score\n",
        "      AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
        "      AUC_collecton.append(AUC)\n",
        "    del model\n",
        "    import torch\n",
        "    import gc\n",
        "    torch.cuda.memory_reserved()\n",
        "    gc.collect()\n",
        "  import statistics\n",
        "  ACC_repeat_col.append(str(round(statistics.mean(ACC_collecton),3))+'±'+ str(round(statistics.stdev(ACC_collecton),3)))\n",
        "  BACC_repeat_col.append(str(round(statistics.mean(BACC_collecton),3))+'±'+str(round(statistics.stdev(BACC_collecton),3)))\n",
        "  Sn_repeat_col.append(str(round(statistics.mean(Sn_collecton),3))+'±'+str(round(statistics.stdev(Sn_collecton),3)))\n",
        "  Sp_repeat_col.append(str(round(statistics.mean(Sp_collecton),3))+'±'+str(round(statistics.stdev(Sp_collecton),3)))\n",
        "  MCC_repeat_col.append(str(round(statistics.mean(MCC_collecton),3))+'±'+str(round(statistics.stdev(MCC_collecton),3)))\n",
        "  AUC_repeat_col.append(str(round(statistics.mean(AUC_collecton),3))+'±'+str(round(statistics.stdev(AUC_collecton),3)))\n",
        "  print(ACC_repeat_col[i],BACC_repeat_col[i],Sn_repeat_col[i],Sp_repeat_col[i],MCC_repeat_col[i],AUC_repeat_col[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvL6bxdaTuOI"
      },
      "outputs": [],
      "source": [
        "# combine the lists into a list of tuples\n",
        "combined_list = list(zip(ACC_repeat_col, BACC_repeat_col, Sn_repeat_col, Sp_repeat_col, MCC_repeat_col, AUC_repeat_col))\n",
        "\n",
        "# create a DataFrame from the list of tuples\n",
        "df = pd.DataFrame(combined_list, columns=['ACC_collection', 'BACC_collection', 'Sn_collection', 'Sp_collection', 'MCC_collection', 'AUC_collection'])\n",
        "\n",
        "# export the DataFrame to an Excel file\n",
        "df.to_excel('AllerStat_480.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Borpedo25DsU"
      },
      "source": [
        "### 640 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20n37i_BXlcv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87357019-b38c-4952-81ab-60e5b875e8b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1 1 1\n"
          ]
        }
      ],
      "source": [
        "CNN_channel = [32] # set as the best parameters\n",
        "dense_node = [4096]\n",
        "kernel_size = [9]\n",
        "stride_size = [2]\n",
        "print(len(CNN_channel),len(dense_node), len(kernel_size),len(stride_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDSA4L8B5DsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa70ccff-cf71-4e16-b717-106bf422418a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t30_150M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t30_150M_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t30_150M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t30_150M_UR50D-contact-regression.pt\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n",
            "0.917±0.008 0.917±0.008 0.924±0.017 0.911±0.014 0.834±0.015 0.972±0.005\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
        "\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('AllerTOPv2.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings_640(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('whole_sample_dataset_esm2_t30_150M_UR50D_unified_640_dimension.csv')\n",
        "\n",
        "# loading the y dataset for model development \n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data_name = 'whole_sample_dataset_esm2_t30_150M_UR50D_unified_640_dimension.csv'\n",
        "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "ACC_repeat_col=[]\n",
        "BACC_repeat_col = []\n",
        "Sn_repeat_col= []\n",
        "Sp_repeat_col=[]\n",
        "MCC_repeat_col=[]\n",
        "AUC_repeat_col=[]\n",
        "\n",
        "for i in range(len(CNN_channel)):\n",
        "  # split dataset as training and test dataset as ratio of 8:2\n",
        "  ACC_collecton = []\n",
        "  BACC_collecton = []\n",
        "  Sn_collecton = []\n",
        "  Sp_collecton = []\n",
        "  MCC_collecton = []\n",
        "  AUC_collecton = []\n",
        "  for random_num in range(10):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=random_num)\n",
        "\n",
        "    # normalize the X data range\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    inputShape=(640,1) # input feature size \n",
        "    input = Input(inputShape)\n",
        "    x = Conv1D(CNN_channel[i],(kernel_size[i]),strides = (stride_size[i]),name='layer_conv2',padding='same')(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(dense_node[i],activation = 'relu',name='fc1')(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "    model = Model(inputs = input,outputs = x,name='Predict')\n",
        "    # define SGD optimizer\n",
        "    momentum = 0.5\n",
        "    sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "    # compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "    # learning deccay setting\n",
        "    import math\n",
        "    def step_decay(epoch): # gradually decrease the learning rate \n",
        "        initial_lrate=0.1\n",
        "        drop=0.6\n",
        "        epochs_drop = 3.0\n",
        "        lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "              math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "        return lrate\n",
        "    lr = LearningRateScheduler(step_decay)\n",
        "    # early stop setting\n",
        "    early_stop = EarlyStopping(monitor='val_accuracy', patience = 40,verbose=0,restore_best_weights = True)\n",
        "    # set checkpoint and save the best model\n",
        "    mc = ModelCheckpoint('best_modelbest_640_repeat_10.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
        "    # summary the callbacks_list\n",
        "    callbacks_list = [ lr , early_stop, mc]\n",
        "    model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 16, verbose=0)\n",
        "    # load the save best model\n",
        "    saved_model = load_model('best_modelbest_640_repeat_10.h5')\n",
        "    # result collection list\n",
        "    # confusion matrix \n",
        "    predicted_class= []\n",
        "    predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "    for p in range(predicted_protability.shape[0]):\n",
        "      index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "      predicted_class.append(index)\n",
        "    predicted_class = np.array(predicted_class)\n",
        "    y_true = y_test    \n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import math\n",
        "    # np.ravel() return a flatten 1D array\n",
        "    TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "    if ACC>0.6:\n",
        "      ACC_collecton.append(ACC)\n",
        "      Sn_collecton.append(TP/(TP+FN))\n",
        "      Sp_collecton.append(TN/(TN+FP))\n",
        "      MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
        "      MCC_collecton.append(MCC)\n",
        "      BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
        "      from sklearn.metrics import roc_auc_score\n",
        "      AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
        "      AUC_collecton.append(AUC)\n",
        "    del model\n",
        "    import torch\n",
        "    import gc\n",
        "    torch.cuda.memory_reserved()\n",
        "    gc.collect()\n",
        "  import statistics\n",
        "  ACC_repeat_col.append(str(round(statistics.mean(ACC_collecton),3))+'±'+ str(round(statistics.stdev(ACC_collecton),3)))\n",
        "  BACC_repeat_col.append(str(round(statistics.mean(BACC_collecton),3))+'±'+str(round(statistics.stdev(BACC_collecton),3)))\n",
        "  Sn_repeat_col.append(str(round(statistics.mean(Sn_collecton),3))+'±'+str(round(statistics.stdev(Sn_collecton),3)))\n",
        "  Sp_repeat_col.append(str(round(statistics.mean(Sp_collecton),3))+'±'+str(round(statistics.stdev(Sp_collecton),3)))\n",
        "  MCC_repeat_col.append(str(round(statistics.mean(MCC_collecton),3))+'±'+str(round(statistics.stdev(MCC_collecton),3)))\n",
        "  AUC_repeat_col.append(str(round(statistics.mean(AUC_collecton),3))+'±'+str(round(statistics.stdev(AUC_collecton),3)))\n",
        "  print(ACC_repeat_col[i],BACC_repeat_col[i],Sn_repeat_col[i],Sp_repeat_col[i],MCC_repeat_col[i],AUC_repeat_col[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN9jCFh-XEwQ"
      },
      "outputs": [],
      "source": [
        "# combine the lists into a list of tuples\n",
        "combined_list = list(zip(ACC_repeat_col, BACC_repeat_col, Sn_repeat_col, Sp_repeat_col, MCC_repeat_col, AUC_repeat_col))\n",
        "\n",
        "# create a DataFrame from the list of tuples\n",
        "df = pd.DataFrame(combined_list, columns=['ACC_collection', 'BACC_collection', 'Sn_collection', 'Sp_collection', 'MCC_collection', 'AUC_collection'])\n",
        "\n",
        "# export the DataFrame to an Excel file\n",
        "df.to_excel('640 ten times repeat performance output_small_contained.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yomxbi7-mNqs"
      },
      "outputs": [],
      "source": [
        "# combine the lists into a list of tuples\n",
        "combined_list = list(zip(ACC_repeat_col, BACC_repeat_col, Sn_repeat_col, Sp_repeat_col, MCC_repeat_col, AUC_repeat_col))\n",
        "\n",
        "# create a DataFrame from the list of tuples\n",
        "df = pd.DataFrame(combined_list, columns=['ACC_collection', 'BACC_collection', 'Sn_collection', 'Sp_collection', 'MCC_collection', 'AUC_collection'])\n",
        "\n",
        "# export the DataFrame to an Excel file\n",
        "df.to_excel('AllerStat_640.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shaxu0Gh5DsV"
      },
      "source": [
        "### 1280 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyrbnM7lM-bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b99de9-17d8-47d3-e50b-eaa28590fafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1 1 1\n"
          ]
        }
      ],
      "source": [
        "CNN_channel = [32] # set as the best parameters\n",
        "dense_node = [1024]\n",
        "kernel_size = [9]\n",
        "stride_size = [8]\n",
        "print(len(CNN_channel),len(dense_node), len(kernel_size),len(stride_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwJXiDEN5DsV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28a0fa29-dcc4-43aa-ee97-fbe5d275fb7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n",
            "0.928±0.01 0.928±0.01 0.93±0.01 0.926±0.012 0.856±0.019 0.976±0.004\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('AllerTOPv2.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "\n",
        "# # generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings_1280(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('whole_sample_dataset_esm2_t33_650M_UR50D_unified_1280_dimension.csv')\n",
        "\n",
        "# loading the y dataset for model development \n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data_name = 'whole_sample_dataset_esm2_t33_650M_UR50D_unified_1280_dimension.csv'\n",
        "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "ACC_repeat_col=[]\n",
        "BACC_repeat_col = []\n",
        "Sn_repeat_col= []\n",
        "Sp_repeat_col=[]\n",
        "MCC_repeat_col=[]\n",
        "AUC_repeat_col=[]\n",
        "\n",
        "for i in range(len(CNN_channel)):\n",
        "  # split dataset as training and test dataset as ratio of 8:2\n",
        "  ACC_collecton = []\n",
        "  BACC_collecton = []\n",
        "  Sn_collecton = []\n",
        "  Sp_collecton = []\n",
        "  MCC_collecton = []\n",
        "  AUC_collecton = []\n",
        "  for random_num in range(10):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=random_num)\n",
        "\n",
        "    # normalize the X data range\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    inputShape=(1280,1) # input feature size \n",
        "    input = Input(inputShape)\n",
        "    x = Conv1D(CNN_channel[i],(kernel_size[i]),strides = (stride_size[i]),name='layer_conv2',padding='same')(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(dense_node[i],activation = 'relu',name='fc1')(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "    model = Model(inputs = input,outputs = x,name='Predict')\n",
        "    # define SGD optimizer\n",
        "    momentum = 0.5\n",
        "    sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "    # compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "    # learning deccay setting\n",
        "    import math\n",
        "    def step_decay(epoch): # gradually decrease the learning rate \n",
        "        initial_lrate=0.1\n",
        "        drop=0.6\n",
        "        epochs_drop = 3.0\n",
        "        lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "              math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "        return lrate\n",
        "    lr = LearningRateScheduler(step_decay)\n",
        "    # early stop setting\n",
        "    early_stop = EarlyStopping(monitor='val_accuracy', patience = 40,verbose=0,restore_best_weights = True)\n",
        "    # set checkpoint and save the best model\n",
        "    mc = ModelCheckpoint('best_modelbest_1280_repeat_10.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
        "    # summary the callbacks_list\n",
        "    callbacks_list = [ lr , early_stop, mc]\n",
        "    model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 16, verbose=0)\n",
        "    # load the save best model\n",
        "    saved_model = load_model('best_modelbest_1280_repeat_10.h5')\n",
        "    # result collection list\n",
        "    # confusion matrix \n",
        "    predicted_class= []\n",
        "    predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "    for p in range(predicted_protability.shape[0]):\n",
        "      index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "      predicted_class.append(index)\n",
        "    predicted_class = np.array(predicted_class)\n",
        "    y_true = y_test    \n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import math\n",
        "    # np.ravel() return a flatten 1D array\n",
        "    TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "    if ACC > 0.6:\n",
        "      ACC_collecton.append(ACC)\n",
        "      Sn_collecton.append(TP/(TP+FN))\n",
        "      Sp_collecton.append(TN/(TN+FP))\n",
        "      MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
        "      MCC_collecton.append(MCC)\n",
        "      BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
        "      from sklearn.metrics import roc_auc_score\n",
        "      AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
        "      AUC_collecton.append(AUC)\n",
        "    del model\n",
        "    import torch\n",
        "    import gc\n",
        "    torch.cuda.memory_reserved()\n",
        "    gc.collect()\n",
        "  import statistics\n",
        "  ACC_repeat_col.append(str(round(statistics.mean(ACC_collecton),3))+'±'+ str(round(statistics.stdev(ACC_collecton),3)))\n",
        "  BACC_repeat_col.append(str(round(statistics.mean(BACC_collecton),3))+'±'+str(round(statistics.stdev(BACC_collecton),3)))\n",
        "  Sn_repeat_col.append(str(round(statistics.mean(Sn_collecton),3))+'±'+str(round(statistics.stdev(Sn_collecton),3)))\n",
        "  Sp_repeat_col.append(str(round(statistics.mean(Sp_collecton),3))+'±'+str(round(statistics.stdev(Sp_collecton),3)))\n",
        "  MCC_repeat_col.append(str(round(statistics.mean(MCC_collecton),3))+'±'+str(round(statistics.stdev(MCC_collecton),3)))\n",
        "  AUC_repeat_col.append(str(round(statistics.mean(AUC_collecton),3))+'±'+str(round(statistics.stdev(AUC_collecton),3)))\n",
        "  print(ACC_repeat_col[i],BACC_repeat_col[i],Sn_repeat_col[i],Sp_repeat_col[i],MCC_repeat_col[i],AUC_repeat_col[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjEJXogC5DsY"
      },
      "outputs": [],
      "source": [
        "# combine the lists into a list of tuples\n",
        "combined_list = list(zip(ACC_repeat_col, BACC_repeat_col, Sn_repeat_col, Sp_repeat_col, MCC_repeat_col, AUC_repeat_col))\n",
        "\n",
        "# create a DataFrame from the list of tuples\n",
        "df = pd.DataFrame(combined_list, columns=['ACC_collection', 'BACC_collection', 'Sn_collection', 'Sp_collection', 'MCC_collection', 'AUC_collection'])\n",
        "\n",
        "# export the DataFrame to an Excel file\n",
        "df.to_excel('AllerStat_1280.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqp51vUi5DsY"
      },
      "source": [
        "### 2560 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-37G3e_LsUYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae5b3b19-b208-40ce-a509-3ac4cde3a255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1 1 1\n"
          ]
        }
      ],
      "source": [
        "# find our the best performance under each node units\n",
        "CNN_channel = [64] # set as the best parameters\n",
        "dense_node = [4096]\n",
        "kernel_size = [6]\n",
        "stride_size = [4]\n",
        "print(len(CNN_channel),len(dense_node), len(kernel_size),len(stride_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7WRDzu85DsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fe60e1a-5fcf-4bd8-f310-df637560ed66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t36_3B_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t36_3B_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t36_3B_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t36_3B_UR50D-contact-regression.pt\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "971/971 [==============================] - 1s 1ms/step\n",
            "0.928±0.009 0.928±0.009 0.932±0.012 0.925±0.013 0.856±0.019 0.977±0.004\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t36_3B_UR50D()\n",
        "\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('AllerTOPv2.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings_2560(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('whole_sample_dataset_esm2_t36_3B_UR50D_unified_2560_dimension.csv')\n",
        "\n",
        "# loading the y dataset for model development \n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data_name = 'whole_sample_dataset_esm2_t36_3B_UR50D_unified_2560_dimension.csv'\n",
        "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "ACC_repeat_col=[]\n",
        "BACC_repeat_col = []\n",
        "Sn_repeat_col= []\n",
        "Sp_repeat_col=[]\n",
        "MCC_repeat_col=[]\n",
        "AUC_repeat_col=[]\n",
        "\n",
        "for i in range(len(CNN_channel)):\n",
        "  # split dataset as training and test dataset as ratio of 8:2\n",
        "  ACC_collecton = []\n",
        "  BACC_collecton = []\n",
        "  Sn_collecton = []\n",
        "  Sp_collecton = []\n",
        "  MCC_collecton = []\n",
        "  AUC_collecton = []\n",
        "  for random_num in range(10):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=random_num)\n",
        "\n",
        "    # normalize the X data range\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    inputShape=(2560,1) # input feature size \n",
        "    input = Input(inputShape)\n",
        "    x = Conv1D(CNN_channel[i],(kernel_size[i]),strides = (stride_size[i]),name='layer_conv2',padding='same')(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(dense_node[i],activation = 'relu',name='fc1')(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "    model = Model(inputs = input,outputs = x,name='Predict')\n",
        "    # define SGD optimizer\n",
        "    momentum = 0.5\n",
        "    sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "    # compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "    # learning deccay setting\n",
        "    import math\n",
        "    def step_decay(epoch): # gradually decrease the learning rate \n",
        "        initial_lrate=0.1\n",
        "        drop=0.6\n",
        "        epochs_drop = 3.0\n",
        "        lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "              math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "        return lrate\n",
        "    lr = LearningRateScheduler(step_decay)\n",
        "    # early stop setting\n",
        "    early_stop = EarlyStopping(monitor='val_accuracy', patience = 40,verbose=0,restore_best_weights = True)\n",
        "    # set checkpoint and save the best model\n",
        "    mc = ModelCheckpoint('best_modelbest_2560_repeat.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
        "    # summary the callbacks_list\n",
        "    callbacks_list = [ lr , early_stop, mc]\n",
        "    model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 16, verbose=0)\n",
        "    # load the save best model\n",
        "    saved_model = load_model('best_modelbest_2560_repeat.h5')\n",
        "    # result collection list\n",
        "    # confusion matrix \n",
        "    predicted_class= []\n",
        "    predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "    for p in range(predicted_protability.shape[0]):\n",
        "      index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "      predicted_class.append(index)\n",
        "    predicted_class = np.array(predicted_class)\n",
        "    y_true = y_test    \n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import math\n",
        "    # np.ravel() return a flatten 1D array\n",
        "    TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "    if ACC>0.6:\n",
        "      ACC_collecton.append(ACC)\n",
        "      Sn_collecton.append(TP/(TP+FN))\n",
        "      Sp_collecton.append(TN/(TN+FP))\n",
        "      MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
        "      MCC_collecton.append(MCC)\n",
        "      BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
        "      from sklearn.metrics import roc_auc_score\n",
        "      AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
        "      AUC_collecton.append(AUC)\n",
        "  import statistics\n",
        "  ACC_repeat_col.append(str(round(statistics.mean(ACC_collecton),3))+'±'+ str(round(statistics.stdev(ACC_collecton),3)))\n",
        "  BACC_repeat_col.append(str(round(statistics.mean(BACC_collecton),3))+'±'+str(round(statistics.stdev(BACC_collecton),3)))\n",
        "  Sn_repeat_col.append(str(round(statistics.mean(Sn_collecton),3))+'±'+str(round(statistics.stdev(Sn_collecton),3)))\n",
        "  Sp_repeat_col.append(str(round(statistics.mean(Sp_collecton),3))+'±'+str(round(statistics.stdev(Sp_collecton),3)))\n",
        "  MCC_repeat_col.append(str(round(statistics.mean(MCC_collecton),3))+'±'+str(round(statistics.stdev(MCC_collecton),3)))\n",
        "  AUC_repeat_col.append(str(round(statistics.mean(AUC_collecton),3))+'±'+str(round(statistics.stdev(AUC_collecton),3)))\n",
        "  print(ACC_repeat_col[i],BACC_repeat_col[i],Sn_repeat_col[i],Sp_repeat_col[i],MCC_repeat_col[i],AUC_repeat_col[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09egNPo8fwxr"
      },
      "outputs": [],
      "source": [
        "# combine the lists into a list of tuples\n",
        "combined_list = list(zip(ACC_repeat_col, BACC_repeat_col, Sn_repeat_col, Sp_repeat_col, MCC_repeat_col, AUC_repeat_col))\n",
        "\n",
        "# create a DataFrame from the list of tuples\n",
        "df = pd.DataFrame(combined_list, columns=['ACC_collection', 'BACC_collection', 'Sn_collection', 'Sp_collection', 'MCC_collection', 'AUC_collection'])\n",
        "\n",
        "# export the DataFrame to an Excel file\n",
        "df.to_excel('AllerStat_2560.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv *h5 /content/drive/MyDrive/universal_allergenicity_new\n",
        "!mv *xlsx /content/drive/MyDrive/universal_allergenicity_new\n",
        "!mv *csv /content/drive/MyDrive/universal_allergenicity_new"
      ],
      "metadata": {
        "id": "AOAYhtErYIYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJpYM1azgR_c",
        "outputId": "0f99550e-8e0e-4bb3-aceb-5b17ff96d499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y993lTuvgPLX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "95NTckuFZZzm",
        "m91cA0H5w_eY",
        "hz-ZO_mrnSsn",
        "dEdAluvxYzfJ",
        "Ex7esuYyY0vU"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}