{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95NTckuFZZzm"
      },
      "source": [
        "## package installation and load packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO71IBS6ZgZV"
      },
      "outputs": [],
      "source": [
        "### packages required \n",
        "!pip install fair-esm \n",
        "!pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install sklearn\n",
        "!pip install h5py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import esm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
        "from keras.layers import Dropout, AveragePooling1D, MaxPooling1D\n",
        "from keras.models import Sequential,Model, load_model\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint,LearningRateScheduler, EarlyStopping\n",
        "import keras\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name():\n",
        "    print('GPU found')\n",
        "    tf.config.experimental.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU') # set the deep learning with GPU \n",
        "else:\n",
        "    print(\"No GPU found\")"
      ],
      "metadata": {
        "id": "w3sxRX-RfzoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m91cA0H5w_eY"
      },
      "source": [
        "### peptide embeddings with differen pretrained model\n",
        "https://github.com/facebookresearch/esm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explaination of the memeory usage of the following models\n",
        "\n",
        "sequence length > 900 \n",
        "\n",
        "2560 output dimension model might need 24 G GPU memory\n",
        "\n",
        "5129 output dimension model, (in our attempts, 40 GB GPU memory is not enough) \n"
      ],
      "metadata": {
        "id": "bmb2VZKeZlWC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl7XVx5HZsHf"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_320(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t6_8M_UR50D' only has 6 layers, and therefore repr_layers parameters is equal to 6\n",
        "      results = esm2(batch_tokens, repr_layers=[6], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][6].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXIH6LOsV-6v"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_480(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t12_35M_UR50D' only has 12 layers, and therefore repr_layers parameters is equal to 12\n",
        "      results = esm2(batch_tokens, repr_layers=[12], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][12].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN7A3bgzhFny"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_640(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t30_150M_UR50D' only has 30 layers, and therefore repr_layers parameters is equal to 30\n",
        "      results = esm2(batch_tokens, repr_layers=[30], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][30].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdQIPEe2V3lB"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_1280(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t33_650M_UR50D' only has 33 layers, and therefore repr_layers parameters is equal to 33\n",
        "      results = esm2(batch_tokens, repr_layers=[33], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][33].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmr0OfcTWN05"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_2560(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t36_3B_UR50D' only has 36 layers, and therefore repr_layers parameters is equal to 36\n",
        "      results = esm2(batch_tokens, repr_layers=[36], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][36].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kjrdZscWULt"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings_5120(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t48_15B_UR50D' only has 48 layers, and therefore repr_layers parameters is equal to 48\n",
        "      results = esm2(batch_tokens, repr_layers=[48], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][48].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz-ZO_mrnSsn"
      },
      "source": [
        "### connect with googledrive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FroNV0ZpnW0e",
        "outputId": "83f0faff-b8cd-4e2d-f765-7bdc387be9cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/universal_allergenicity_new')"
      ],
      "metadata": {
        "id": "zOCDoACdowFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6JdFa_7ZGL7",
        "outputId": "aea8c398-257f-41d7-9151-23394f3f2233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/universal_allergenicity_new\n",
            "allergens_dataset.xlsx\n",
            "whole_sample_dataset_esm2_t12_35M_UR50D_unified_480_dimension.csv\n",
            "whole_sample_dataset_esm2_t30_150M_UR50D_unified_640_dimension.csv\n",
            "whole_sample_dataset_esm2_t6_8M_UR50D_unified_320_dimension.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJ-0YPyiQhp"
      },
      "source": [
        "#### load packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTetST5AiUAh"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
        "from keras.layers import Dropout, AveragePooling1D, MaxPooling1D\n",
        "from keras.models import Sequential,Model, load_model\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint,LearningRateScheduler, EarlyStopping\n",
        "import keras\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name():\n",
        "    print('GPU found')\n",
        "    tf.config.experimental.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU') # set the deep learning with GPU \n",
        "else:\n",
        "    print(\"No GPU found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RLR10hUsxca"
      },
      "source": [
        "## Sequence embeddings with different pretrained protein language models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztgr6bl-YGDI"
      },
      "source": [
        "### 320 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eogcsymmYJ6A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
        "\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings_320(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('whole_sample_dataset_esm2_t6_8M_UR50D_unified_320_dimension.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3yzGDFEYfCY"
      },
      "source": [
        "### 480 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SMpqoc-3YlCo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
        "\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings_480(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('whole_sample_dataset_esm2_t12_35M_UR50D_unified_480_dimension.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFyOy-19Yvil"
      },
      "source": [
        "### 640 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TXYWJ5EY4o-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
        "\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings_640(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('whole_sample_dataset_esm2_t30_150M_UR50D_unified_640_dimension.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEdAluvxYzfJ"
      },
      "source": [
        "### 1280 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwang7k8Y_gt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings_1280(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('whole_sample_dataset_esm2_t33_650M_UR50D_unified_1280_dimension.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex7esuYyY0vU"
      },
      "source": [
        "### 2560 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRUBeL3uaG0a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t36_3B_UR50D()\n",
        "\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "a=0\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings_2560(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "    a=a+1\n",
        "    print(a)\n",
        "embeddings_results.to_csv('whole_sample_dataset_esm2_t36_3B_UR50D_unified_2560_dimension.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## grid search for hyperparameters of CNN model\n",
        "Due to the enormous memory consumption and limitted dataset, we do not test 256 fileter size in CNN with 8192 units in dense layer."
      ],
      "metadata": {
        "id": "xSCXCjv8gYIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 320 feature dimension embedding test"
      ],
      "metadata": {
        "id": "jZBMpsS3_Ymu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "\n",
        "# loading the y dataset for model development \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data_name = 'whole_sample_dataset_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
        "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "# split dataset as training and test dataset as ratio of 8:2\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# normalize the X data range\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "ACC_collecton = []\n",
        "CNN_channel = [16,32,64,128,256]\n",
        "dense_node = [32,64,128,256,512,1024,2048,4096,8192]\n",
        "kernel_size = [3,6,9,12]\n",
        "stride_size = [1,2,4,8]\n",
        "for i in range(len(CNN_channel)):\n",
        "  for j in range(len(dense_node)):\n",
        "    for k in range(len(kernel_size)):\n",
        "      for l in range(len(stride_size)):\n",
        "        inputShape=(320,1) # input feature size \n",
        "        input = Input(inputShape)\n",
        "        x = Conv1D(CNN_channel[i],(kernel_size[k]),strides = (stride_size[l]),name='layer_conv2',padding='same')(input)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "        x = Dropout(0.15)(x)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(dense_node[j],activation = 'relu',name='fc1')(x)\n",
        "        x = Dropout(0.15)(x)\n",
        "        x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "        model = Model(inputs = input,outputs = x,name='Predict')\n",
        "        # define SGD optimizer\n",
        "        momentum = 0.5\n",
        "        sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "        # compile the model\n",
        "        model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "        # learning deccay setting\n",
        "        import math\n",
        "        def step_decay(epoch): # gradually decrease the learning rate \n",
        "            initial_lrate=0.1\n",
        "            drop=0.6\n",
        "            epochs_drop = 3.0\n",
        "            lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "                  math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "            return lrate\n",
        "        lr = LearningRateScheduler(step_decay)\n",
        "        # early stop setting\n",
        "        early_stop = EarlyStopping(monitor='val_accuracy', patience = 20,verbose=0,restore_best_weights = True)\n",
        "        # set checkpoint and save the best model\n",
        "        mc = ModelCheckpoint('best_model_grid_320.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
        "        # summary the callbacks_list\n",
        "        callbacks_list = [ lr , early_stop, mc]\n",
        "        model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 32, verbose=0)\n",
        "        # load the save best model\n",
        "        saved_model = load_model('best_model_grid_320.h5')\n",
        "        # result collection list\n",
        "        # confusion matrix \n",
        "        predicted_class= []\n",
        "        predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "        for p in range(predicted_protability.shape[0]):\n",
        "          index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "          predicted_class.append(index)\n",
        "        predicted_class = np.array(predicted_class)\n",
        "        y_true = y_test    \n",
        "        from sklearn.metrics import confusion_matrix\n",
        "        import math\n",
        "        # np.ravel() return a flatten 1D array\n",
        "        TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "        ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "        ACC_collecton.append(ACC)\n",
        "        print(ACC)\n",
        "        del model\n",
        "        import torch\n",
        "        import gc\n",
        "        torch.cuda.memory_reserved()\n",
        "        gc.collect()\n",
        "\n",
        "import collections\n",
        "model_parameters =collections.defaultdict(list)\n",
        "a=0\n",
        "for i in range(len(CNN_channel)):\n",
        "  for j in range(len(dense_node)):\n",
        "    for k in range(len(kernel_size)):\n",
        "      for l in range(len(stride_size)):\n",
        "        iter_num = str(a)\n",
        "        model_parameters[iter_num].append(CNN_channel[i])\n",
        "        model_parameters[iter_num].append(dense_node[j])\n",
        "        model_parameters[iter_num].append(kernel_size[k])\n",
        "        model_parameters[iter_num].append(stride_size[l])\n",
        "        model_parameters[iter_num].append(ACC_collecton[a])\n",
        "        a=a+1\n",
        "# export the DataFrame to an Excel file\n",
        "pd.DataFrame(model_parameters).to_excel('320_grid_performance output.xlsx', index=False)"
      ],
      "metadata": {
        "id": "sguq72SHkfOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 480 feature dimension embedding test"
      ],
      "metadata": {
        "id": "Vn_ddIHq_Ymv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "\n",
        "# loading the y dataset for model development \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data_name = 'whole_sample_dataset_esm2_t12_35M_UR50D_unified_480_dimension.csv'\n",
        "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "# split dataset as training and test dataset as ratio of 8:2\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# normalize the X data range\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "ACC_collecton = []\n",
        "CNN_channel = [16,32,64,128,256]\n",
        "dense_node = [32,64,128,256,512,1024,2048,4096,8192]\n",
        "kernel_size = [3,6,9,12]\n",
        "stride_size = [1,2,4,8]\n",
        "for i in range(len(CNN_channel)):\n",
        "  for j in range(len(dense_node)):\n",
        "    for k in range(len(kernel_size)):\n",
        "      for l in range(len(stride_size)):\n",
        "        inputShape=(480,1) # input feature size \n",
        "        input = Input(inputShape)\n",
        "        x = Conv1D(CNN_channel[i],(kernel_size[k]),strides = (stride_size[l]),name='layer_conv2',padding='same')(input)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "        x = Dropout(0.15)(x)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(dense_node[j],activation = 'relu',name='fc1')(x)\n",
        "        x = Dropout(0.15)(x)\n",
        "        x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "        model = Model(inputs = input,outputs = x,name='Predict')\n",
        "        # define SGD optimizer\n",
        "        momentum = 0.5\n",
        "        sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "        # compile the model\n",
        "        model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "        # learning deccay setting\n",
        "        import math\n",
        "        def step_decay(epoch): # gradually decrease the learning rate \n",
        "            initial_lrate=0.1\n",
        "            drop=0.6\n",
        "            epochs_drop = 3.0\n",
        "            lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "                  math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "            return lrate\n",
        "        lr = LearningRateScheduler(step_decay)\n",
        "        # early stop setting\n",
        "        early_stop = EarlyStopping(monitor='val_accuracy', patience = 20,verbose=0,restore_best_weights = True)\n",
        "        # set checkpoint and save the best model\n",
        "        mc = ModelCheckpoint('best_model_grid_480.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
        "        # summary the callbacks_list\n",
        "        callbacks_list = [ lr , early_stop, mc]\n",
        "        model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 32, verbose=0)\n",
        "        # load the save best model\n",
        "        saved_model = load_model('best_model_grid_480.h5')\n",
        "        # result collection list\n",
        "        # confusion matrix \n",
        "        predicted_class= []\n",
        "        predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "        for p in range(predicted_protability.shape[0]):\n",
        "          index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "          predicted_class.append(index)\n",
        "        predicted_class = np.array(predicted_class)\n",
        "        y_true = y_test    \n",
        "        from sklearn.metrics import confusion_matrix\n",
        "        import math\n",
        "        # np.ravel() return a flatten 1D array\n",
        "        TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "        ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "        ACC_collecton.append(ACC)\n",
        "        print(ACC)\n",
        "        del model\n",
        "        import torch\n",
        "        import gc\n",
        "        torch.cuda.memory_reserved()\n",
        "        gc.collect()\n",
        "\n",
        "import collections\n",
        "model_parameters =collections.defaultdict(list)\n",
        "a=0\n",
        "for i in range(len(CNN_channel)):\n",
        "  for j in range(len(dense_node)):\n",
        "    for k in range(len(kernel_size)):\n",
        "      for l in range(len(stride_size)):\n",
        "        iter_num = str(a)\n",
        "        model_parameters[iter_num].append(CNN_channel[i])\n",
        "        model_parameters[iter_num].append(dense_node[j])\n",
        "        model_parameters[iter_num].append(kernel_size[k])\n",
        "        model_parameters[iter_num].append(stride_size[l])\n",
        "        model_parameters[iter_num].append(ACC_collecton[a])\n",
        "        a=a+1\n",
        "# export the DataFrame to an Excel file\n",
        "pd.DataFrame(model_parameters).to_excel('480_grid_performance output.xlsx', index=False)"
      ],
      "metadata": {
        "id": "V6YpKLSZkUo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 640 feature dimension embedding test"
      ],
      "metadata": {
        "id": "HXbl2M2w_Ymw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "\n",
        "# loading the y dataset for model development \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data_name = 'whole_sample_dataset_esm2_t30_150M_UR50D_unified_640_dimension.csv'\n",
        "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "# split dataset as training and test dataset as ratio of 8:2\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# normalize the X data range\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "ACC_collecton = []\n",
        "CNN_channel = [16,32,64,128,256]\n",
        "dense_node = [32,64,128,256,512,1024,2048,4096,8192]\n",
        "kernel_size = [3,6,9,12]\n",
        "stride_size = [1,2,4,8]\n",
        "for i in range(len(CNN_channel)):\n",
        "  for j in range(len(dense_node)):\n",
        "    for k in range(len(kernel_size)):\n",
        "      for l in range(len(stride_size)):\n",
        "        inputShape=(640,1) # input feature size \n",
        "        input = Input(inputShape)\n",
        "        x = Conv1D(CNN_channel[i],(kernel_size[k]),strides = (stride_size[l]),name='layer_conv2',padding='same')(input)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "        x = Dropout(0.15)(x)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(dense_node[j],activation = 'relu',name='fc1')(x)\n",
        "        x = Dropout(0.15)(x)\n",
        "        x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "        model = Model(inputs = input,outputs = x,name='Predict')\n",
        "        # define SGD optimizer\n",
        "        momentum = 0.5\n",
        "        sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "        # compile the model\n",
        "        model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "        # learning deccay setting\n",
        "        import math\n",
        "        def step_decay(epoch): # gradually decrease the learning rate \n",
        "            initial_lrate=0.1\n",
        "            drop=0.6\n",
        "            epochs_drop = 3.0\n",
        "            lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "                  math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "            return lrate\n",
        "        lr = LearningRateScheduler(step_decay)\n",
        "        # early stop setting\n",
        "        early_stop = EarlyStopping(monitor='val_accuracy', patience = 20,verbose=0,restore_best_weights = True)\n",
        "        # set checkpoint and save the best model\n",
        "        mc = ModelCheckpoint('best_model_grid_640.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
        "        # summary the callbacks_list\n",
        "        callbacks_list = [ lr , early_stop, mc]\n",
        "        model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 32, verbose=0)\n",
        "        # load the save best model\n",
        "        saved_model = load_model('best_model_grid_640.h5')\n",
        "        # result collection list\n",
        "        # confusion matrix \n",
        "        predicted_class= []\n",
        "        predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "        for p in range(predicted_protability.shape[0]):\n",
        "          index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "          predicted_class.append(index)\n",
        "        predicted_class = np.array(predicted_class)\n",
        "        y_true = y_test    \n",
        "        from sklearn.metrics import confusion_matrix\n",
        "        import math\n",
        "        # np.ravel() return a flatten 1D array\n",
        "        TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "        ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "        ACC_collecton.append(ACC)\n",
        "        print(ACC)\n",
        "        del model\n",
        "        import torch\n",
        "        import gc\n",
        "        torch.cuda.memory_reserved()\n",
        "        gc.collect()\n",
        "import collections\n",
        "model_parameters =collections.defaultdict(list)\n",
        "a=0\n",
        "for i in range(len(CNN_channel)):\n",
        "  for j in range(len(dense_node)):\n",
        "    for k in range(len(kernel_size)):\n",
        "      for l in range(len(stride_size)):\n",
        "        iter_num = str(a)\n",
        "        model_parameters[iter_num].append(CNN_channel[i])\n",
        "        model_parameters[iter_num].append(dense_node[j])\n",
        "        model_parameters[iter_num].append(kernel_size[k])\n",
        "        model_parameters[iter_num].append(stride_size[l])\n",
        "        model_parameters[iter_num].append(ACC_collecton[a])\n",
        "        a=a+1\n",
        "# export the DataFrame to an Excel file\n",
        "pd.DataFrame(model_parameters).to_excel('640_grid_performance output.xlsx', index=False)"
      ],
      "metadata": {
        "id": "8J_Da_KbkPgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1280 feature dimension embedding test\n"
      ],
      "metadata": {
        "id": "riQQE4SS_Ymy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "\n",
        "# loading the y dataset for model development \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data_name = 'whole_sample_dataset_esm2_t33_650M_UR50D_unified_1280_dimension.csv'\n",
        "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "# split dataset as training and test dataset as ratio of 8:2\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# normalize the X data range\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "ACC_collecton = []\n",
        "CNN_channel = [16,32,64,128,256]\n",
        "dense_node = [32,64,128,256,512,1024,2048,4096,8192]\n",
        "kernel_size = [3,6,9,12]\n",
        "stride_size = [1,2,4,8]\n",
        "for i in range(len(CNN_channel)):\n",
        "  for j in range(len(dense_node)):\n",
        "    for k in range(len(kernel_size)):\n",
        "      for l in range(len(stride_size)):\n",
        "        inputShape=(1280,1) # input feature size \n",
        "        input = Input(inputShape)\n",
        "        x = Conv1D(CNN_channel[i],(kernel_size[k]),strides = (stride_size[l]),name='layer_conv2',padding='same')(input)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "        x = Dropout(0.15)(x)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(dense_node[j],activation = 'relu',name='fc1')(x)\n",
        "        x = Dropout(0.15)(x)\n",
        "        x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "        model = Model(inputs = input,outputs = x,name='Predict')\n",
        "        # define SGD optimizer\n",
        "        momentum = 0.5\n",
        "        sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "        # compile the model\n",
        "        model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "        # learning deccay setting\n",
        "        import math\n",
        "        def step_decay(epoch): # gradually decrease the learning rate \n",
        "            initial_lrate=0.1\n",
        "            drop=0.6\n",
        "            epochs_drop = 3.0\n",
        "            lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "                  math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "            return lrate\n",
        "        lr = LearningRateScheduler(step_decay)\n",
        "        # early stop setting\n",
        "        early_stop = EarlyStopping(monitor='val_accuracy', patience = 20,verbose=0,restore_best_weights = True)\n",
        "        # set checkpoint and save the best model\n",
        "        mc = ModelCheckpoint('best_model_grid_1280.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
        "        # summary the callbacks_list\n",
        "        callbacks_list = [ lr , early_stop, mc]\n",
        "        model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 32, verbose=0)\n",
        "        # load the save best model\n",
        "        saved_model = load_model('best_model_grid_1280.h5')\n",
        "        # result collection list\n",
        "        # confusion matrix \n",
        "        predicted_class= []\n",
        "        predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "        for p in range(predicted_protability.shape[0]):\n",
        "          index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "          predicted_class.append(index)\n",
        "        predicted_class = np.array(predicted_class)\n",
        "        y_true = y_test    \n",
        "        from sklearn.metrics import confusion_matrix\n",
        "        import math\n",
        "        # np.ravel() return a flatten 1D array\n",
        "        TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "        ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "        ACC_collecton.append(ACC)\n",
        "        print(ACC)\n",
        "        del model\n",
        "        import torch\n",
        "        import gc\n",
        "        torch.cuda.memory_reserved()\n",
        "        gc.collect()\n",
        "import collections\n",
        "model_parameters =collections.defaultdict(list)\n",
        "a=0\n",
        "for i in range(len(CNN_channel)):\n",
        "  for j in range(len(dense_node)):\n",
        "    for k in range(len(kernel_size)):\n",
        "      for l in range(len(stride_size)):\n",
        "        iter_num = str(a)\n",
        "        model_parameters[iter_num].append(CNN_channel[i])\n",
        "        model_parameters[iter_num].append(dense_node[j])\n",
        "        model_parameters[iter_num].append(kernel_size[k])\n",
        "        model_parameters[iter_num].append(stride_size[l])\n",
        "        model_parameters[iter_num].append(ACC_collecton[a])\n",
        "        a=a+1\n",
        "# export the DataFrame to an Excel file\n",
        "pd.DataFrame(model_parameters).to_excel('1280_grid_performance output.xlsx', index=False)"
      ],
      "metadata": {
        "id": "LaLj2PIgckpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2560 feature dimension embedding test\n",
        "Due to the enourmous computational needs and also the limitted performance improivement, we do not test the 128 and 256 fileter size with 8196 units."
      ],
      "metadata": {
        "id": "A8q0KSsR_Ymz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "\n",
        "# loading the y dataset for model development \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data_name = 'whole_sample_dataset_esm2_t36_3B_UR50D_unified_2560_dimension.csv'\n",
        "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "# split dataset as training and test dataset as ratio of 8:2\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# normalize the X data range\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "ACC_collecton = []\n",
        "CNN_channel = [16,32,64,128,256] # \n",
        "dense_node = [32,64,128,256,512,1024,2048,4096]\n",
        "kernel_size = [3,6,9,12]\n",
        "stride_size = [1,2,4,8]\n",
        "for i in range(len(CNN_channel)):\n",
        "  for j in range(len(dense_node)):\n",
        "    for k in range(len(kernel_size)):\n",
        "      for l in range(len(stride_size)):\n",
        "        inputShape=(2560,1) # input feature size \n",
        "        input = Input(inputShape)\n",
        "        x = Conv1D(CNN_channel[i],(kernel_size[k]),strides = (stride_size[l]),name='layer_conv2',padding='same')(input)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "        x = Dropout(0.15)(x)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(dense_node[j],activation = 'relu',name='fc1')(x)\n",
        "        x = Dropout(0.15)(x)\n",
        "        x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "        model = Model(inputs = input,outputs = x,name='Predict')\n",
        "        # define SGD optimizer\n",
        "        momentum = 0.5\n",
        "        sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "        # compile the model\n",
        "        model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "        # learning deccay setting\n",
        "        import math\n",
        "        def step_decay(epoch): # gradually decrease the learning rate \n",
        "            initial_lrate=0.1\n",
        "            drop=0.6\n",
        "            epochs_drop = 3.0\n",
        "            lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "                  math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "            return lrate\n",
        "        lr = LearningRateScheduler(step_decay)\n",
        "        # early stop setting\n",
        "        early_stop = EarlyStopping(monitor='val_accuracy', patience = 20,verbose=0,restore_best_weights = True)\n",
        "        # set checkpoint and save the best model\n",
        "        mc = ModelCheckpoint('best_model_grid_2560.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
        "        # summary the callbacks_list\n",
        "        callbacks_list = [ lr , early_stop, mc]\n",
        "        model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 32, verbose=0)\n",
        "        # load the save best model\n",
        "        saved_model = load_model('best_model_grid_2560.h5')\n",
        "        # result collection list\n",
        "        # confusion matrix \n",
        "        predicted_class= []\n",
        "        predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "        for p in range(predicted_protability.shape[0]):\n",
        "          index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "          predicted_class.append(index)\n",
        "        predicted_class = np.array(predicted_class)\n",
        "        y_true = y_test    \n",
        "        from sklearn.metrics import confusion_matrix\n",
        "        import math\n",
        "        # np.ravel() return a flatten 1D array\n",
        "        TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "        ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "        ACC_collecton.append(ACC)\n",
        "        print(ACC)\n",
        "        del model\n",
        "        import torch\n",
        "        import gc\n",
        "        torch.cuda.memory_reserved()\n",
        "        gc.collect()\n",
        "import collections\n",
        "model_parameters =collections.defaultdict(list)\n",
        "a=0\n",
        "for i in range(len(CNN_channel)):\n",
        "  for j in range(len(dense_node)):\n",
        "    for k in range(len(kernel_size)):\n",
        "      for l in range(len(stride_size)):\n",
        "        iter_num = str(a)\n",
        "        model_parameters[iter_num].append(CNN_channel[i])\n",
        "        model_parameters[iter_num].append(dense_node[j])\n",
        "        model_parameters[iter_num].append(kernel_size[k])\n",
        "        model_parameters[iter_num].append(stride_size[l])\n",
        "        model_parameters[iter_num].append(ACC_collecton[a])\n",
        "        a=a+1\n",
        "# export the DataFrame to an Excel file\n",
        "pd.DataFrame(model_parameters).to_excel('2560_grid_performance output.xlsx', index=False)"
      ],
      "metadata": {
        "id": "Xk1v0R7hCRFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 times random dataset division test"
      ],
      "metadata": {
        "id": "tkgD_C1RDwWh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAMCI-1F5DsR"
      },
      "source": [
        "### 320 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the best parameters\n",
        "CNN_channel = [32, 64, 16, 32, 32, 16, 16, 32, 32, 32, 128, 64, 32, 32, 64, 64, 128, 32, 64, 16, 128, 32, 16, 32, 32, 32]\n",
        "dense_node = [32, 32, 64, 64, 128, 128, 256, 256, 512, 512, 8192, 4096, 8192, 4096, 4096, 4096, 4096, 4096, 8192, 1024, 2048, 2048, 2048, 1024, 512, 512]\n",
        "kernel_size = [9, 6, 1, 6, 12, 12, 12, 3, 3, 9, 3, 12, 9, 6, 12, 12, 12, 12, 12, 9, 12, 12, 6, 6, 3, 12]\n",
        "stride_size = [1, 4, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 4, 4, 1, 2, 1, 8, 1, 1, 1, 1, 2]\n",
        "print(len(CNN_channel),len(dense_node), len(kernel_size),len(stride_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6jNTjB1Mskh",
        "outputId": "286ae310-abb7-4967-ed1a-cf55fdc3f801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26 26 26 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NJab0Rm5DsR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data_name = 'whole_sample_dataset_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
        "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "ACC_repeat_col=[]\n",
        "BACC_repeat_col = []\n",
        "Sn_repeat_col= []\n",
        "Sp_repeat_col=[]\n",
        "MCC_repeat_col=[]\n",
        "AUC_repeat_col=[]\n",
        "\n",
        "for i in range(len(CNN_channel)):\n",
        "  # split dataset as training and test dataset as ratio of 8:2\n",
        "  ACC_collecton = []\n",
        "  BACC_collecton = []\n",
        "  Sn_collecton = []\n",
        "  Sp_collecton = []\n",
        "  MCC_collecton = []\n",
        "  AUC_collecton = []\n",
        "  for random_num in range(10):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=random_num)\n",
        "\n",
        "    # normalize the X data range\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    inputShape=(320,1) # input feature size \n",
        "    input = Input(inputShape)\n",
        "    x = Conv1D(CNN_channel[i],(kernel_size[i]),strides = (stride_size[i]),name='layer_conv2',padding='same')(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(dense_node[i],activation = 'relu',name='fc1')(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "    model = Model(inputs = input,outputs = x,name='Predict')\n",
        "    # define SGD optimizer\n",
        "    momentum = 0.5\n",
        "    sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "    # compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "    # learning deccay setting\n",
        "    import math\n",
        "    def step_decay(epoch): # gradually decrease the learning rate \n",
        "        initial_lrate=0.1\n",
        "        drop=0.6\n",
        "        epochs_drop = 3.0\n",
        "        lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "              math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "        return lrate\n",
        "    lr = LearningRateScheduler(step_decay)\n",
        "    # early stop setting\n",
        "    early_stop = EarlyStopping(monitor='val_accuracy', patience = 40,verbose=0,restore_best_weights = True)\n",
        "    # set checkpoint and save the best model\n",
        "    mc = ModelCheckpoint('best_modelbest_320_10times.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
        "    # summary the callbacks_list\n",
        "    callbacks_list = [ lr , early_stop, mc]\n",
        "    model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 16, verbose=0)\n",
        "    # load the save best model\n",
        "    saved_model = load_model('best_modelbest_320_10times.h5')\n",
        "    # result collection list\n",
        "    # confusion matrix \n",
        "    predicted_class= []\n",
        "    predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "    for p in range(predicted_protability.shape[0]):\n",
        "      index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "      predicted_class.append(index)\n",
        "    predicted_class = np.array(predicted_class)\n",
        "    y_true = y_test    \n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import math\n",
        "    # np.ravel() return a flatten 1D array\n",
        "    TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "    ACC_collecton.append(ACC)\n",
        "    Sn_collecton.append(TP/(TP+FN))\n",
        "    Sp_collecton.append(TN/(TN+FP))\n",
        "    MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
        "    MCC_collecton.append(MCC)\n",
        "    BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
        "    AUC_collecton.append(AUC)\n",
        "    del model\n",
        "    import torch\n",
        "    import gc\n",
        "    torch.cuda.memory_reserved()\n",
        "    gc.collect()\n",
        "  import statistics\n",
        "  ACC_repeat_col.append(str(round(statistics.mean(ACC_collecton),3))+''+ str(round(statistics.stdev(ACC_collecton),3)))\n",
        "  BACC_repeat_col.append(str(round(statistics.mean(BACC_collecton),3))+''+str(round(statistics.stdev(BACC_collecton),3)))\n",
        "  Sn_repeat_col.append(str(round(statistics.mean(Sn_collecton),3))+''+str(round(statistics.stdev(Sn_collecton),3)))\n",
        "  Sp_repeat_col.append(str(round(statistics.mean(Sp_collecton),3))+''+str(round(statistics.stdev(Sp_collecton),3)))\n",
        "  MCC_repeat_col.append(str(round(statistics.mean(MCC_collecton),3))+''+str(round(statistics.stdev(MCC_collecton),3)))\n",
        "  AUC_repeat_col.append(str(round(statistics.mean(AUC_collecton),3))+''+str(round(statistics.stdev(AUC_collecton),3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCRm5lh5SRH7"
      },
      "outputs": [],
      "source": [
        "# combine the lists into a list of tuples\n",
        "combined_list = list(zip(ACC_repeat_col, BACC_repeat_col, Sn_repeat_col, Sp_repeat_col, MCC_repeat_col, AUC_repeat_col))\n",
        "\n",
        "# create a DataFrame from the list of tuples\n",
        "df = pd.DataFrame(combined_list, columns=['ACC_collection', 'BACC_collection', 'Sn_collection', 'Sp_collection', 'MCC_collection', 'AUC_collection'])\n",
        "\n",
        "# export the DataFrame to an Excel file\n",
        "df.to_excel('320 ten times repeat performance output.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVsxPkQe5DsT"
      },
      "source": [
        "### 480 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set as the best parameters\n",
        "CNN_channel = [128, 32, 64, 64, 32, 16, 16, 16, 16, 16, 64, 64, 64, 16, 128, 16, 32, 32, 16, 32, 32, 64, 32, 32, 16, 16, 32, 32, 16, 32]\n",
        "dense_node = [8192, 4096, 8192, 8192, 4096, 8192, 8192, 2048, 8192, 2048, 2048, 2048, 1024, 2048, 1024, 2048, 512, 1024, 512, 32, 32, 64, 64, 128, 128, 256, 256, 512, 512, 512]\n",
        "kernel_size = [9, 6, 6, 9, 9, 12, 6, 12, 9, 9, 6, 12, 12, 6, 12, 9, 6, 12, 12, 6, 12, 6, 9, 9, 12, 6, 9, 6, 12, 6]\n",
        "stride_size = [8, 1, 4, 8, 4, 2, 2, 2, 2, 2, 8, 4, 4, 1, 4, 1, 4, 1, 1, 1, 1, 2, 4, 1, 1, 2, 1, 4, 1, 2]\n",
        "print(len(CNN_channel),len(dense_node), len(kernel_size),len(stride_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eulq3XK_McQv",
        "outputId": "7d70529f-8b20-475d-b62c-bf8168ef90b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 30 30 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N54VN0CY5DsT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "# # generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "\n",
        "# loading the y dataset for model development \n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data_name = 'whole_sample_dataset_esm2_t12_35M_UR50D_unified_480_dimension.csv'\n",
        "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "ACC_repeat_col=[]\n",
        "BACC_repeat_col = []\n",
        "Sn_repeat_col= []\n",
        "Sp_repeat_col=[]\n",
        "MCC_repeat_col=[]\n",
        "AUC_repeat_col=[]\n",
        "for i in range(len(CNN_channel)):\n",
        "  # split dataset as training and test dataset as ratio of 8:2\n",
        "  ACC_collecton = []\n",
        "  BACC_collecton = []\n",
        "  Sn_collecton = []\n",
        "  Sp_collecton = []\n",
        "  MCC_collecton = []\n",
        "  AUC_collecton = []\n",
        "  for random_num in range(10):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=random_num)\n",
        "\n",
        "    # normalize the X data range\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    inputShape=(480,1) # input feature size \n",
        "    input = Input(inputShape)\n",
        "    x = Conv1D(CNN_channel[i],(kernel_size[i]),strides = (stride_size[i]),name='layer_conv2',padding='same')(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(dense_node[i],activation = 'relu',name='fc1')(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "    model = Model(inputs = input,outputs = x,name='Predict')\n",
        "    # define SGD optimizer\n",
        "    momentum = 0.5\n",
        "    sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "    # compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "    # learning deccay setting\n",
        "    import math\n",
        "    def step_decay(epoch): # gradually decrease the learning rate \n",
        "        initial_lrate=0.1\n",
        "        drop=0.6\n",
        "        epochs_drop = 3.0\n",
        "        lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "              math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "        return lrate\n",
        "    lr = LearningRateScheduler(step_decay)\n",
        "    # early stop setting\n",
        "    early_stop = EarlyStopping(monitor='val_accuracy', patience = 40,verbose=0,restore_best_weights = True)\n",
        "    # set checkpoint and save the best model\n",
        "    mc = ModelCheckpoint('best_modelbest_480_repeat_1.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
        "    # summary the callbacks_list\n",
        "    callbacks_list = [ lr , early_stop, mc]\n",
        "    model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 16, verbose=0)\n",
        "    # load the save best model\n",
        "    saved_model = load_model('best_modelbest_480_repeat_1.h5')\n",
        "    # result collection list\n",
        "    # confusion matrix \n",
        "    predicted_class= []\n",
        "    predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "    for p in range(predicted_protability.shape[0]):\n",
        "      index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "      predicted_class.append(index)\n",
        "    predicted_class = np.array(predicted_class)\n",
        "    y_true = y_test    \n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import math\n",
        "    # np.ravel() return a flatten 1D array\n",
        "    TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "    ACC_collecton.append(ACC)\n",
        "    Sn_collecton.append(TP/(TP+FN))\n",
        "    Sp_collecton.append(TN/(TN+FP))\n",
        "    MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
        "    MCC_collecton.append(MCC)\n",
        "    BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
        "    AUC_collecton.append(AUC)\n",
        "    del model\n",
        "    import torch\n",
        "    import gc\n",
        "    torch.cuda.memory_reserved()\n",
        "    gc.collect()\n",
        "  import statistics\n",
        "  ACC_repeat_col.append(str(round(statistics.mean(ACC_collecton),3))+''+ str(round(statistics.stdev(ACC_collecton),3)))\n",
        "  BACC_repeat_col.append(str(round(statistics.mean(BACC_collecton),3))+''+str(round(statistics.stdev(BACC_collecton),3)))\n",
        "  Sn_repeat_col.append(str(round(statistics.mean(Sn_collecton),3))+''+str(round(statistics.stdev(Sn_collecton),3)))\n",
        "  Sp_repeat_col.append(str(round(statistics.mean(Sp_collecton),3))+''+str(round(statistics.stdev(Sp_collecton),3)))\n",
        "  MCC_repeat_col.append(str(round(statistics.mean(MCC_collecton),3))+''+str(round(statistics.stdev(MCC_collecton),3)))\n",
        "  AUC_repeat_col.append(str(round(statistics.mean(AUC_collecton),3))+''+str(round(statistics.stdev(AUC_collecton),3)))\n",
        "  print(ACC_repeat_col[i],BACC_repeat_col[i],Sn_repeat_col[i],Sp_repeat_col[i],MCC_repeat_col[i],AUC_repeat_col[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvL6bxdaTuOI"
      },
      "outputs": [],
      "source": [
        "# combine the lists into a list of tuples\n",
        "combined_list = list(zip(ACC_repeat_col, BACC_repeat_col, Sn_repeat_col, Sp_repeat_col, MCC_repeat_col, AUC_repeat_col))\n",
        "\n",
        "# create a DataFrame from the list of tuples\n",
        "df = pd.DataFrame(combined_list, columns=['ACC_collection', 'BACC_collection', 'Sn_collection', 'Sp_collection', 'MCC_collection', 'AUC_collection'])\n",
        "\n",
        "# export the DataFrame to an Excel file\n",
        "df.to_excel('480 ten times repeat performance output.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Borpedo25DsU"
      },
      "source": [
        "### 640 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the best parameters\n",
        "CNN_channel = [64, 64, 64, 32, 64, 16, 128, 64, 32, 32, 32, 64, 32, 16, 128, 16, 128, 64, 64, 64, 16, 64, 64, 16, 32, 128, 32, 16, 32, 64, 32, 64, 64, 64]\n",
        "dense_node = [8192, 2048, 8192, 8192, 4096, 2048, 4096, 2048, 2048, 4096, 512, 1024, 4096, 2048, 4096, 1024, 512, 512, 1024, 512, 128, 1024, 512, 32, 32, 64, 64, 128, 128, 256, 256, 512, 512, 512]\n",
        "kernel_size = [12, 12, 6, 9, 12, 12, 6, 9, 9, 9, 9, 9, 12, 9, 9, 12, 9, 9, 9, 12, 9, 6, 9, 12, 9, 12, 9, 9, 9, 9, 6, 9, 12, 9]\n",
        "stride_size = [2, 4, 4, 2, 2, 2, 4, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2]\n",
        "print(len(CNN_channel),len(dense_node), len(kernel_size),len(stride_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNhMhGuAMFyQ",
        "outputId": "3d6fa164-f77b-464f-c3bf-01212d33ea29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34 34 34 34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yDSA4L8B5DsU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "# loading the y dataset for model development \n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data_name = 'whole_sample_dataset_esm2_t30_150M_UR50D_unified_640_dimension.csv'\n",
        "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "ACC_repeat_col=[]\n",
        "BACC_repeat_col = []\n",
        "Sn_repeat_col= []\n",
        "Sp_repeat_col=[]\n",
        "MCC_repeat_col=[]\n",
        "AUC_repeat_col=[]\n",
        "\n",
        "for i in range(len(CNN_channel)):\n",
        "  # split dataset as training and test dataset as ratio of 8:2\n",
        "  ACC_collecton = []\n",
        "  BACC_collecton = []\n",
        "  Sn_collecton = []\n",
        "  Sp_collecton = []\n",
        "  MCC_collecton = []\n",
        "  AUC_collecton = []\n",
        "  for random_num in range(10):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=random_num)\n",
        "\n",
        "    # normalize the X data range\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    inputShape=(640,1) # input feature size \n",
        "    input = Input(inputShape)\n",
        "    x = Conv1D(CNN_channel[i],(kernel_size[i]),strides = (stride_size[i]),name='layer_conv2',padding='same')(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(dense_node[i],activation = 'relu',name='fc1')(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "    model = Model(inputs = input,outputs = x,name='Predict')\n",
        "    # define SGD optimizer\n",
        "    momentum = 0.5\n",
        "    sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "    # compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "    # learning deccay setting\n",
        "    import math\n",
        "    def step_decay(epoch): # gradually decrease the learning rate \n",
        "        initial_lrate=0.1\n",
        "        drop=0.6\n",
        "        epochs_drop = 3.0\n",
        "        lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "              math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "        return lrate\n",
        "    lr = LearningRateScheduler(step_decay)\n",
        "    # early stop setting\n",
        "    early_stop = EarlyStopping(monitor='val_accuracy', patience = 40,verbose=0,restore_best_weights = True)\n",
        "    # set checkpoint and save the best model\n",
        "    mc = ModelCheckpoint('best_modelbest_640_repeat_10.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
        "    # summary the callbacks_list\n",
        "    callbacks_list = [ lr , early_stop, mc]\n",
        "    model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 16, verbose=0)\n",
        "    # load the save best model\n",
        "    saved_model = load_model('best_modelbest_640_repeat_10.h5')\n",
        "    # result collection list\n",
        "    # confusion matrix \n",
        "    predicted_class= []\n",
        "    predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "    for p in range(predicted_protability.shape[0]):\n",
        "      index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "      predicted_class.append(index)\n",
        "    predicted_class = np.array(predicted_class)\n",
        "    y_true = y_test    \n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import math\n",
        "    # np.ravel() return a flatten 1D array\n",
        "    TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "    ACC_collecton.append(ACC)\n",
        "    Sn_collecton.append(TP/(TP+FN))\n",
        "    Sp_collecton.append(TN/(TN+FP))\n",
        "    MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
        "    MCC_collecton.append(MCC)\n",
        "    BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
        "    AUC_collecton.append(AUC)\n",
        "    del model\n",
        "    import torch\n",
        "    import gc\n",
        "    torch.cuda.memory_reserved()\n",
        "    gc.collect()\n",
        "  import statistics\n",
        "  ACC_repeat_col.append(str(round(statistics.mean(ACC_collecton),3))+''+ str(round(statistics.stdev(ACC_collecton),3)))\n",
        "  BACC_repeat_col.append(str(round(statistics.mean(BACC_collecton),3))+''+str(round(statistics.stdev(BACC_collecton),3)))\n",
        "  Sn_repeat_col.append(str(round(statistics.mean(Sn_collecton),3))+''+str(round(statistics.stdev(Sn_collecton),3)))\n",
        "  Sp_repeat_col.append(str(round(statistics.mean(Sp_collecton),3))+''+str(round(statistics.stdev(Sp_collecton),3)))\n",
        "  MCC_repeat_col.append(str(round(statistics.mean(MCC_collecton),3))+''+str(round(statistics.stdev(MCC_collecton),3)))\n",
        "  AUC_repeat_col.append(str(round(statistics.mean(AUC_collecton),3))+''+str(round(statistics.stdev(AUC_collecton),3)))\n",
        "  print(ACC_repeat_col[i],BACC_repeat_col[i],Sn_repeat_col[i],Sp_repeat_col[i],MCC_repeat_col[i],AUC_repeat_col[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN9jCFh-XEwQ"
      },
      "outputs": [],
      "source": [
        "# combine the lists into a list of tuples\n",
        "combined_list = list(zip(ACC_repeat_col, BACC_repeat_col, Sn_repeat_col, Sp_repeat_col, MCC_repeat_col, AUC_repeat_col))\n",
        "\n",
        "# create a DataFrame from the list of tuples\n",
        "df = pd.DataFrame(combined_list, columns=['ACC_collection', 'BACC_collection', 'Sn_collection', 'Sp_collection', 'MCC_collection', 'AUC_collection'])\n",
        "\n",
        "# export the DataFrame to an Excel file\n",
        "df.to_excel('640 ten times repeat performance output.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shaxu0Gh5DsV"
      },
      "source": [
        "### 1280 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyrbnM7lM-bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e21cf56-5030-48b0-c606-1813bda6defa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42 42 42 42\n"
          ]
        }
      ],
      "source": [
        "# set as the best parameters\n",
        "CNN_channel = [32, 128, 32, 32, 32, 16, 64, 16, 16, 64, 64, 64, 43, 128, 64, 16, 128, 64, 128, 16, 128, 16, 64, 16, 16, 256, 32, 16, 32, 128, 16, 32, 32, 64, 256, 128, 128, 16, 64, 32, 128, 256]\n",
        "dense_node = [8192, 4096, 1024, 8192, 2048, 1024, 4096, 512, 8192, 2048, 512, 256, 512, 1024, 1024, 2048, 4096, 1024, 512, 128, 1024, 1024, 512, 256, 32, 32, 32, 64, 64, 64, 128, 128, 128, 256, 256, 256, 256, 512, 512, 512, 512, 256]\n",
        "kernel_size = [9, 12, 9, 9, 6, 12, 9, 12, 12, 9, 9, 12, 6, 12, 9, 12, 12, 12, 12, 12, 12, 6, 12, 3, 12, 6, 12, 6, 12, 12, 12, 12, 12, 12, 9, 9, 9, 12, 9, 6, 12, 9]\n",
        "stride_size = [2, 8, 8, 8, 1, 2, 2, 1, 2, 2, 8, 2, 8, 1, 2, 2, 2, 4, 8, 1, 2, 4, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 8, 2, 1, 8, 8, 8, 1]\n",
        "print(len(CNN_channel),len(dense_node), len(kernel_size),len(stride_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwJXiDEN5DsV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "# # generate the peptide embeddings\n",
        "sequence_list = dataset['sequence'] \n",
        "\n",
        "# loading the y dataset for model development \n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data_name = 'whole_sample_dataset_esm2_t33_650M_UR50D_unified_1280_dimension.csv'\n",
        "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "ACC_repeat_col=[]\n",
        "BACC_repeat_col = []\n",
        "Sn_repeat_col= []\n",
        "Sp_repeat_col=[]\n",
        "MCC_repeat_col=[]\n",
        "AUC_repeat_col=[]\n",
        "\n",
        "for i in range(len(CNN_channel)):\n",
        "  # split dataset as training and test dataset as ratio of 8:2\n",
        "  ACC_collecton = []\n",
        "  BACC_collecton = []\n",
        "  Sn_collecton = []\n",
        "  Sp_collecton = []\n",
        "  MCC_collecton = []\n",
        "  AUC_collecton = []\n",
        "  for random_num in range(10):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=random_num)\n",
        "\n",
        "    # normalize the X data range\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    inputShape=(1280,1) # input feature size \n",
        "    input = Input(inputShape)\n",
        "    x = Conv1D(CNN_channel[i],(kernel_size[i]),strides = (stride_size[i]),name='layer_conv2',padding='same')(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(dense_node[i],activation = 'relu',name='fc1')(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "    model = Model(inputs = input,outputs = x,name='Predict')\n",
        "    # define SGD optimizer\n",
        "    momentum = 0.5\n",
        "    sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "    # compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "    # learning deccay setting\n",
        "    import math\n",
        "    def step_decay(epoch): # gradually decrease the learning rate \n",
        "        initial_lrate=0.1\n",
        "        drop=0.6\n",
        "        epochs_drop = 3.0\n",
        "        lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "              math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "        return lrate\n",
        "    lr = LearningRateScheduler(step_decay)\n",
        "    # early stop setting\n",
        "    early_stop = EarlyStopping(monitor='val_accuracy', patience = 40,verbose=0,restore_best_weights = True)\n",
        "    # set checkpoint and save the best model\n",
        "    mc = ModelCheckpoint('best_modelbest_1280_repeat_10.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
        "    # summary the callbacks_list\n",
        "    callbacks_list = [ lr , early_stop, mc]\n",
        "    model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 16, verbose=0)\n",
        "    # load the save best model\n",
        "    saved_model = load_model('best_modelbest_1280_repeat_10.h5')\n",
        "    # result collection list\n",
        "    # confusion matrix \n",
        "    predicted_class= []\n",
        "    predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "    for p in range(predicted_protability.shape[0]):\n",
        "      index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "      predicted_class.append(index)\n",
        "    predicted_class = np.array(predicted_class)\n",
        "    y_true = y_test    \n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import math\n",
        "    # np.ravel() return a flatten 1D array\n",
        "    TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "    ACC_collecton.append(ACC)\n",
        "    Sn_collecton.append(TP/(TP+FN))\n",
        "    Sp_collecton.append(TN/(TN+FP))\n",
        "    MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
        "    MCC_collecton.append(MCC)\n",
        "    BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
        "    AUC_collecton.append(AUC)\n",
        "    del model\n",
        "    import torch\n",
        "    import gc\n",
        "    torch.cuda.memory_reserved()\n",
        "    gc.collect()\n",
        "  import statistics\n",
        "  ACC_repeat_col.append(str(round(statistics.mean(ACC_collecton),3))+''+ str(round(statistics.stdev(ACC_collecton),3)))\n",
        "  BACC_repeat_col.append(str(round(statistics.mean(BACC_collecton),3))+''+str(round(statistics.stdev(BACC_collecton),3)))\n",
        "  Sn_repeat_col.append(str(round(statistics.mean(Sn_collecton),3))+''+str(round(statistics.stdev(Sn_collecton),3)))\n",
        "  Sp_repeat_col.append(str(round(statistics.mean(Sp_collecton),3))+''+str(round(statistics.stdev(Sp_collecton),3)))\n",
        "  MCC_repeat_col.append(str(round(statistics.mean(MCC_collecton),3))+''+str(round(statistics.stdev(MCC_collecton),3)))\n",
        "  AUC_repeat_col.append(str(round(statistics.mean(AUC_collecton),3))+''+str(round(statistics.stdev(AUC_collecton),3)))\n",
        "  print(ACC_repeat_col[i],BACC_repeat_col[i],Sn_repeat_col[i],Sp_repeat_col[i],MCC_repeat_col[i],AUC_repeat_col[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjEJXogC5DsY"
      },
      "outputs": [],
      "source": [
        "# combine the lists into a list of tuples\n",
        "combined_list = list(zip(ACC_repeat_col, BACC_repeat_col, Sn_repeat_col, Sp_repeat_col, MCC_repeat_col, AUC_repeat_col))\n",
        "\n",
        "# create a DataFrame from the list of tuples\n",
        "df = pd.DataFrame(combined_list, columns=['ACC_collection', 'BACC_collection', 'Sn_collection', 'Sp_collection', 'MCC_collection', 'AUC_collection'])\n",
        "\n",
        "# export the DataFrame to an Excel file\n",
        "df.to_excel('1280 10 times repeat performance output.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqp51vUi5DsY"
      },
      "source": [
        "### 2560 feature dimension embedding test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set best parameters\n",
        "CNN_channel = [128, 64, 64, 16, 64, 128, 32, 128, 64, 32, 32, 128, 64, 16, 32, 16, 32, 64, 16, 64, 32, 32, 16, 64, 32, 32, 64, 128, 256, 32, 32, 16, 16, 16, 32, 16, 16]\n",
        "dense_node = [2048, 2048, 4096, 512, 4096, 1024, 2048, 4096, 1024, 256, 2048, 1024, 128, 4096, 4096, 32, 32, 32, 32, 64, 64, 64, 64, 128, 128, 128, 128, 256, 256, 256, 256, 256, 512, 512, 512, 512, 512]\n",
        "kernel_size = [6, 9, 12, 6, 6, 9, 6, 12, 9, 12, 12, 6, 9, 6, 3, 6, 12, 9, 3, 3, 9, 9, 3, 6, 6, 6, 9, 6, 9, 9, 12, 3, 6, 6, 6, 6, 3]\n",
        "stride_size = [4, 8, 8, 4, 4, 8, 8, 8, 1, 8, 8, 8, 1, 8, 4, 4, 2, 2, 4, 1, 8, 4, 2, 1, 4, 8, 1, 4, 1, 2, 8, 2, 4, 2, 2, 8, 2]\n",
        "print(len(CNN_channel),len(dense_node), len(kernel_size),len(stride_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO7ZbycBLNYW",
        "outputId": "aa14e5d0-cf72-426a-c883-96782e5d6330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37 37 37 37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7WRDzu85DsY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "# whole dataset loading and dataset splitting \n",
        "dataset = pd.read_excel('allergens_dataset.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "# loading the y dataset for model development \n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data_name = 'whole_sample_dataset_esm2_t36_3B_UR50D_unified_2560_dimension.csv'\n",
        "X_data = pd.read_csv(X_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "ACC_repeat_col=[]\n",
        "BACC_repeat_col = []\n",
        "Sn_repeat_col= []\n",
        "Sp_repeat_col=[]\n",
        "MCC_repeat_col=[]\n",
        "AUC_repeat_col=[]\n",
        "\n",
        "for i in range(len(CNN_channel)):\n",
        "  # split dataset as training and test dataset as ratio of 8:2\n",
        "  ACC_collecton = []\n",
        "  BACC_collecton = []\n",
        "  Sn_collecton = []\n",
        "  Sp_collecton = []\n",
        "  MCC_collecton = []\n",
        "  AUC_collecton = []\n",
        "  for random_num in range(10):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=random_num)\n",
        "\n",
        "    # normalize the X data range\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    inputShape=(2560,1) # input feature size \n",
        "    input = Input(inputShape)\n",
        "    x = Conv1D(CNN_channel[i],(kernel_size[i]),strides = (stride_size[i]),name='layer_conv2',padding='same')(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(dense_node[i],activation = 'relu',name='fc1')(x)\n",
        "    x = Dropout(0.15)(x)\n",
        "    x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "    model = Model(inputs = input,outputs = x,name='Predict')\n",
        "    # define SGD optimizer\n",
        "    momentum = 0.5\n",
        "    sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "    # compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "    # learning deccay setting\n",
        "    import math\n",
        "    def step_decay(epoch): # gradually decrease the learning rate \n",
        "        initial_lrate=0.1\n",
        "        drop=0.6\n",
        "        epochs_drop = 3.0\n",
        "        lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "              math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "        return lrate\n",
        "    lr = LearningRateScheduler(step_decay)\n",
        "    # early stop setting\n",
        "    early_stop = EarlyStopping(monitor='val_accuracy', patience = 40,verbose=0,restore_best_weights = True)\n",
        "    # set checkpoint and save the best model\n",
        "    mc = ModelCheckpoint('best_modelbest_2560_repeat.h5',  monitor='val_accuracy', mode='max', verbose=0, save_best_only=True, save_weights_only=False)\n",
        "    # summary the callbacks_list\n",
        "    callbacks_list = [ lr , early_stop, mc]\n",
        "    model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 16, verbose=0)\n",
        "    # load the save best model\n",
        "    saved_model = load_model('best_modelbest_2560_repeat.h5')\n",
        "    # result collection list\n",
        "    # confusion matrix \n",
        "    predicted_class= []\n",
        "    predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "    for p in range(predicted_protability.shape[0]):\n",
        "      index = np.where(predicted_protability[p] == np.amax(predicted_protability[p]))[0][0]\n",
        "      predicted_class.append(index)\n",
        "    predicted_class = np.array(predicted_class)\n",
        "    y_true = y_test    \n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import math\n",
        "    # np.ravel() return a flatten 1D array\n",
        "    TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "    if ACC>0.6:\n",
        "      ACC_collecton.append(ACC)\n",
        "      Sn_collecton.append(TP/(TP+FN))\n",
        "      Sp_collecton.append(TN/(TN+FP))\n",
        "      MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
        "      MCC_collecton.append(MCC)\n",
        "      BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
        "      from sklearn.metrics import roc_auc_score\n",
        "      AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
        "      AUC_collecton.append(AUC)\n",
        "  import statistics\n",
        "  ACC_repeat_col.append(str(round(statistics.mean(ACC_collecton),3))+''+ str(round(statistics.stdev(ACC_collecton),3)))\n",
        "  BACC_repeat_col.append(str(round(statistics.mean(BACC_collecton),3))+''+str(round(statistics.stdev(BACC_collecton),3)))\n",
        "  Sn_repeat_col.append(str(round(statistics.mean(Sn_collecton),3))+''+str(round(statistics.stdev(Sn_collecton),3)))\n",
        "  Sp_repeat_col.append(str(round(statistics.mean(Sp_collecton),3))+''+str(round(statistics.stdev(Sp_collecton),3)))\n",
        "  MCC_repeat_col.append(str(round(statistics.mean(MCC_collecton),3))+''+str(round(statistics.stdev(MCC_collecton),3)))\n",
        "  AUC_repeat_col.append(str(round(statistics.mean(AUC_collecton),3))+''+str(round(statistics.stdev(AUC_collecton),3)))\n",
        "  print(ACC_repeat_col[i],BACC_repeat_col[i],Sn_repeat_col[i],Sp_repeat_col[i],MCC_repeat_col[i],AUC_repeat_col[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09egNPo8fwxr"
      },
      "outputs": [],
      "source": [
        "# combine the lists into a list of tuples\n",
        "combined_list = list(zip(ACC_repeat_col, BACC_repeat_col, Sn_repeat_col, Sp_repeat_col, MCC_repeat_col, AUC_repeat_col))\n",
        "\n",
        "# create a DataFrame from the list of tuples\n",
        "df = pd.DataFrame(combined_list, columns=['ACC_collection', 'BACC_collection', 'Sn_collection', 'Sp_collection', 'MCC_collection', 'AUC_collection'])\n",
        "\n",
        "# export the DataFrame to an Excel file\n",
        "df.to_excel('2560 ten times repeat performance output.xlsx', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "95NTckuFZZzm",
        "m91cA0H5w_eY",
        "5RLR10hUsxca",
        "ztgr6bl-YGDI",
        "C3yzGDFEYfCY",
        "mFyOy-19Yvil",
        "dEdAluvxYzfJ",
        "Ex7esuYyY0vU",
        "jZBMpsS3_Ymu",
        "Vn_ddIHq_Ymv",
        "HXbl2M2w_Ymw",
        "riQQE4SS_Ymy",
        "A8q0KSsR_Ymz",
        "xAMCI-1F5DsR",
        "nVsxPkQe5DsT",
        "Borpedo25DsU",
        "shaxu0Gh5DsV",
        "yqp51vUi5DsY"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}